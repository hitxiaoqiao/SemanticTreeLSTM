{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import external_lib as el\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_glove_path, = el.download_and_unzip(\n",
    "#  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "#  'glove.840B.300d.txt', data_dir = \"./data_sources/glove.6B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'data/sick_filtered_glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove_path = 'data_sources/glove.6B/glove.840B.300d.txt'\n",
    "#el.filter_glove(full_glove_path, filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from data/sick_filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = el.load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])\n",
    "        \n",
    "    def copy(self, parent=None):\n",
    "        root = Node(data.copy())\n",
    "        if parent: \n",
    "            root.add_parent(parent)\n",
    "        for child in self.children:\n",
    "            root.add_child(child.copy(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate copies of the tree with diffrent arrangement orders of children for nodes in generation_to_rearrange\"\"\"\n",
    "\n",
    "def generate_tree_permutations(tree, generation_to_rearrange=1):\n",
    "    target_generation, target_generation_addresses = get_generation_and_addresses(tree, generation_to_rearrange=1)\n",
    "    tree_perms = []\n",
    "    if target_generation: \n",
    "        for i, parent in enumerate(target_generation):\n",
    "            if len(parent.children) > 1:\n",
    "                perms = get_permutations_for(len(parent.children))\n",
    "                for j, perm in enumerate(perms):\n",
    "                    #make a copy of the tree\n",
    "                    #rearrange the children of parent using the func below \n",
    "                    #add the result to tree_perms\n",
    "\n",
    "def get_permutations_for(size):\n",
    "    indexes = list(range(size))\n",
    "    return list(permutations(indexes))\n",
    "    \n",
    "\"\"\"Returns None when there are no possible permutatations in the selected generation\"\"\"\n",
    "def get_generation_and_addresses(tree, generation_to_rearrange=1):\n",
    "    target_generation_addresses = None\n",
    "    target_generation = None\n",
    "    if len(tree.children) < 2 and generation_to_rearrange==0:\n",
    "        return target_generation, target_generation_addresses\n",
    "    elif generation_to_rearrange == 0:\n",
    "        return [tree], [[0]]\n",
    "    \n",
    "    prev_gen = [tree]\n",
    "    prev_gen_addresses = [[0]]\n",
    "    \n",
    "    next_gen = []\n",
    "    next_gen_addresses = []\n",
    "    current_gen = 0\n",
    "    while len(prev_gen)>0:\n",
    "        next_gen = []\n",
    "        next_gen_addresses = []\n",
    "        for i, parent in enumerate(prev_gen):\n",
    "            if len(parent.children)>1:\n",
    "                for j, child in enumerate(parent.children):\n",
    "                    next_gen.append(child)\n",
    "                    parent_address = prev_gen_addresses[i].copy()\n",
    "                    parent_address.append(j)\n",
    "                    next_gen_addresses.append(parent_address)\n",
    "        \n",
    "        current_gen += 1\n",
    "        if current_gen == generation_to_rearrange:\n",
    "            target_generation = next_gen\n",
    "            target_generation_addresses = next_gen_addresses\n",
    "            break\n",
    "        prev_gen = next_gen\n",
    "        prev_gen_addresses = next_gen_addresses\n",
    "        \n",
    "    return target_generation, target_generation_addresses\n",
    "    \n",
    "def rearrange_list(list_, index_order):\n",
    "    list_ = [ list_[i] for i in index_order]\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\n",
    "Parse indented lines of text into a tree. Children are indented & under the parent\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert trees to a linear representation. Children are listed between the left and right \n",
    "marker in front of the parent. Each word is replaced by its id \"\"\"\n",
    "unknown_word = word_to_idx[\"UNKNOWN_WORD\"]\n",
    "left_marker = word_to_idx[\"LEFT_MARKER\"]\n",
    "right_marker = word_to_idx[\"RIGHT_MARKER\"]\n",
    "end_marker = word_to_idx[\"END_MARKER\"]\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        tree_batches.append(tree_batch)\n",
    "        batch = []\n",
    "        batches.append(batch)\n",
    "        batch_lengths = []\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for tree in tree_batch:\n",
    "            result =[]\n",
    "            batch.append(result)\n",
    "            handle_node(tree, result)\n",
    "            batch_lengths.append(len(result))\n",
    "            if len(result) > max_sequence_length:\n",
    "                max_sequence_length = len(result)\n",
    "    \n",
    "    return batches, tree_batches, max_sequence_length,batches_lengths\n",
    "                \n",
    "            \n",
    "def handle_node(node, result):\n",
    "    result.append(left_marker)\n",
    "    word = node.data[0]\n",
    "    if word in word_to_idx:\n",
    "        result.append(word_to_idx[word])\n",
    "    else:\n",
    "        result.append(unknown_word)\n",
    "        #print(\"Unknown word: \"+word)\n",
    "    if len(node.children)>0:\n",
    "        \n",
    "        for child in node.children:\n",
    "            handle_node(child, result)\n",
    "    result.append(right_marker)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pad sequences with end markers\"\"\"\n",
    "def pad_sequences(batches, max_sequence_length):\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length :\n",
    "                sentence.append(end_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert the score into a set of probabilities over the classes\"\"\"\n",
    "\"\"\"Since the loss uses KL Divergence the PMF cannot have 0 values \"\"\"\n",
    "def convert_scores_to_p(scores_list):\n",
    "    scores = np.array(scores_list) \n",
    "    num_of_classes = 5 #1, 2, .. , 4, 5\n",
    "    p = np.zeros((len(scores), num_of_classes))\n",
    "    for i, score in enumerate(scores): \n",
    "        floor = math.floor(score)\n",
    "        if score == num_of_classes:\n",
    "            p[i] = p[i]+0.0001\n",
    "            p[i][num_of_classes-1] = 0.9996\n",
    "        elif floor == score:\n",
    "            p[i] = p[i] + 0.0001\n",
    "            p[i][floor-1] = 0.9996\n",
    "        else:\n",
    "            p[i] = p[i] + 0.0001\n",
    "            p[i][floor] = score - floor - 0.00015 #floor+1-1  zero index adjustment\n",
    "            p[i][floor-1] = floor - score + 1 - 0.00015 #floor-1  zero index adjustment\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the sentences into words and convert the words to their ids\n",
    "The set of words by which to split the sentence can be found in the corresponding tree\n",
    "so fetch the set of words first \"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "def create_sentence_batches(sentences, trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        sentence_batch = sentences[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        batch = []\n",
    "        batch_lengths = []\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for j, tree in enumerate(tree_batch):\n",
    "            word_list =[]\n",
    "            get_word_list(tree, word_list)\n",
    "            #set_trace()\n",
    "            sentence_ids = []\n",
    "            batch.append(sentence_ids)\n",
    "            ordered_word_list = sentence_batch[j].replace(\",\", \" , \").replace(\".\", \" . \").replace(\"n't\", \" n't\").replace(\"'s\", \" 's \").split()\n",
    "            \n",
    "            for k in range(len(ordered_word_list)):\n",
    "                word = ordered_word_list[k]\n",
    "                if not word in word_list:\n",
    "                    print(\"missing word: \" + word)\n",
    "                    set_trace()\n",
    "                    for token in word_list:\n",
    "                        if (not token in ordered_word_list) and token in word:\n",
    "                            words = word.replace(token, \" \"+token+\" \").split()\n",
    "                            for half_word in words:\n",
    "                                if half_word in word_to_idx:\n",
    "                                    sentence_ids.append(word_to_idx[half_word])\n",
    "                                else:\n",
    "                                    sentence_ids.append(unknown_word)\n",
    "                            break\n",
    "                elif word in word_to_idx:\n",
    "                    sentence_ids.append(word_to_idx[word])\n",
    "                else:\n",
    "                    sentence_ids.append(unknown_word)\n",
    "            batch_lengths.append(len(sentence_ids))\n",
    "            if len(sentence_ids) > max_sequence_length:\n",
    "                max_sequence_length = len(sentence_ids)\n",
    "    \n",
    "    return batches, max_sequence_length, batches_lengths\n",
    "                \n",
    "            \n",
    "def get_word_list(node, word_list):\n",
    "    word = node.data[0]\n",
    "    word_list.append(word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            get_word_list(child, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(file):\n",
    "    with open(file, 'r') as f: \n",
    "        contents = f.readlines()\n",
    "        if len(contents[-1].strip())==0:\n",
    "            contents.pop(-1)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_offsets(batch_size, sequence_len, input_lengths):\n",
    "    return np.array(range(batch_size))*sequence_len + input_lengths-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "sequence_len = 100\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 5 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10\n",
    "learn_rate = 0.05\n",
    "max_grad_norm = 5\n",
    "epoch_size = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_train_sentenceA_tree.txt')\n",
    "sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_train_sentenceB_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesA, tree_batchesA, max_sequence_lengthA, seq_lenA = create_batches(sentence_treesA, batch_size)\n",
    "batchesB, tree_batchesB, max_sequence_lengthB, seq_lenB = create_batches(sentence_treesB, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = max(max_sequence_lengthA, max_sequence_lengthB)\n",
    "sequence_len_tensor = tf.constant(sequence_len, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(batchesA, sequence_len)\n",
    "pad_sequences(batchesB, sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores('data/sick_train_score.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "sentence_inputs_A = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_A\")\n",
    "sentence_inputs_A_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_A_length\")\n",
    "serial_index_offsets_A = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_A\")\n",
    "\n",
    "sentence_inputs_B = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_B\")\n",
    "sentence_inputs_B_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_B_length\")\n",
    "serial_index_offsets_B = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_B\")\n",
    "\n",
    "target_score = tf.placeholder(data_type, shape=(None, output_size), name=\"target_scores\")\n",
    "target_score_scalar = tf.placeholder(data_type, shape=(None, ), name=\"target_scores\")\n",
    "\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "embedded_inputs_A = tf.nn.embedding_lookup(embedding, sentence_inputs_A)\n",
    "embedded_inputs_B = tf.nn.embedding_lookup(embedding, sentence_inputs_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def makeCells():\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        cell1 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        cell2 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    return [cell1, cell2]\n",
    "\n",
    "A =makeCells()\n",
    "B =makeCells()\n",
    "with tf.variable_scope(\"UnrolledStackedCells\", reuse=tf.AUTO_REUSE):\n",
    "    cellA = tf.contrib.rnn.MultiRNNCell(A, state_is_tuple=True)\n",
    "    outputsA, final_stateA = tf.nn.dynamic_rnn(cellA, embedded_inputs_A,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_A_length)\n",
    "    \n",
    "    cellB = tf.contrib.rnn.MultiRNNCell(B, state_is_tuple=True)\n",
    "    outputsB, final_stateB = tf.nn.dynamic_rnn(cellB, embedded_inputs_B,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_B_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'UnrolledStackedCells/rnn/transpose_1:0' shape=(?, 108, 300) dtype=float64>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_outputsA = tf.reshape(outputsA, [-1, hidden_size])\n",
    "terminal_outputsA = tf.gather(serialized_outputsA, serial_index_offsets_A)\n",
    "\n",
    "serialized_outputsB = tf.reshape(outputsB, [-1, hidden_size])\n",
    "terminal_outputsB = tf.gather(serialized_outputsB, serial_index_offsets_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.abs(tf.subtract(terminal_outputsA, terminal_outputsB))\n",
    "h_elewise_product = tf.multiply(terminal_outputsA, terminal_outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) \n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = tf.multiply(p_hat, tf.constant([1,2,3,4,5], dtype=tf.float64))\n",
    "y_hat = tf.reduce_sum(y_p, 1)\n",
    "MSE = tf.losses.mean_squared_error(target_score_scalar, y_hat)\n",
    "pMSE = tf.cast(tf.losses.mean_squared_error(target_score, p_hat), data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "p_over_p_hat = tf.div(target_score, p_hat)\n",
    "log_p_over_p_hat = tf.log(p_over_p_hat)\n",
    "KL = tf.reduce_mean(tf.reduce_sum(tf.multiply(target_score, log_p_over_p_hat), 1))\n",
    "regularizer = tf.constant(0.0,dtype=data_type)\n",
    "for var in tf.trainable_variables(): \n",
    "    regularizer = tf.add(regularizer, tf.nn.l2_loss(var))\n",
    "loss = KL + reg_lambda*regularizer\n",
    "\n",
    "learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),\n",
    "                                      max_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "TrainLoss_summary = tf.summary.scalar('TrainLoss', loss)\n",
    "TestLoss_summary = tf.summary.scalar('TestLoss', loss)\n",
    "MSE_summary = tf.summary.scalar('Test MSE', MSE)\n",
    "pMSE_summary = tf.summary.scalar('Test pMSE', pMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(batchesA, batchesB, seq_lenA, seq_lenB, scores, output_file):\n",
    "    output_scores=[]\n",
    "    total_loss = 0\n",
    "    feed_dict = {sentence_inputs_A:np.array(batchesA), sentence_inputs_A_length:np.array(seq_lenA), \n",
    "                 serial_index_offsets_A:create_index_offsets(len(batchesA), sequence_len, seq_lenA),\n",
    "                 sentence_inputs_B:np.array(batchesB), sentence_inputs_B_length:np.array(seq_lenB),\n",
    "                 serial_index_offsets_B:create_index_offsets(len(batchesB), sequence_len, seq_lenB),\n",
    "                 target_score:np.array(convert_scores_to_p(scores)), target_score_scalar:np.array(scores) }\n",
    "\n",
    "\n",
    "    fetches = {'loss': loss, 'y_hat': y_hat, 'TestLoss_summary':TestLoss_summary,\n",
    "              'MSE_summary':MSE_summary, 'pMSE_summary':pMSE_summary}\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    total_loss = vals[\"loss\"]\n",
    "    output_scores.append(vals[\"y_hat\"])\n",
    "\n",
    "\n",
    "    #print(\"Loss %.3f\" % (loss))\n",
    "\n",
    "    #import csv\n",
    "    \"\"\"with open(output_file,'w') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        for batch in output_scores:\n",
    "        for score in batch: \n",
    "            wr.writerow([repr(score)])\"\"\"\n",
    "    return total_loss, vals[\"TestLoss_summary\"], vals[\"MSE_summary\"], vals[\"pMSE_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchesA, test_seq_lenA = None, None\n",
    "test_batchesB, test_seq_lenB = None, None\n",
    "test_scores = None\n",
    "def load_tree_test_data_and_test_model(test_batchesA_, test_seq_lenA_,test_batchesB_, test_seq_lenB_,test_scores_):\n",
    "    global test_batchesA, test_seq_lenA, test_batchesB, test_seq_lenB, test_scores\n",
    "    if test_batchesA_ : \n",
    "        return test_model(test_batchesA_[0], test_batchesB_[0], test_seq_lenA_[0], test_seq_lenB_[0], test_scores_[0], \n",
    "                   \"test_results.txt\")\n",
    "    else:        \n",
    "        sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_trial_sentenceA_tree.txt')\n",
    "        sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_trial_sentenceB_tree.txt')\n",
    "\n",
    "        test_batchesA, test_tree_batchesA, max_sequence_lengthA, test_seq_lenA = create_batches(sentence_treesA,\n",
    "                                                                                                len(sentence_treesA))\n",
    "        test_batchesB, test_tree_batchesB, max_sequence_lengthB, test_seq_lenB = create_batches(sentence_treesB,\n",
    "                                                                                                len(sentence_treesB))\n",
    "\n",
    "        pad_sequences(test_batchesA, sequence_len)\n",
    "        pad_sequences(test_batchesB, sequence_len)\n",
    "\n",
    "        test_scores = load_scores('data/sick_trial_score.txt', len(sentence_treesA))\n",
    "        return test_model(test_batchesA[0], test_batchesB[0], test_seq_lenA[0], test_seq_lenB[0], test_scores[0], \n",
    "                       \"test_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_time = time.time()\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(session, \"./event_and_checkpoints/event_and_checkpoints/SemanticRelatednessLSTM-h.ckpt\")\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./event_and_checkpoints\", session.graph)\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 perplexity: 4.397 speed: 19 wps\n",
      "100*Loss 148.093\n",
      "total_test_loss 1.4404881\n",
      "0.002 perplexity: 3.694 speed: 74 wps\n",
      "100*Loss 118.970\n",
      "total_test_loss 1.2578482\n",
      "0.012 perplexity: 3.153 speed: 142 wps\n",
      "100*Loss 96.660\n",
      "total_test_loss 1.0785980\n",
      "0.022 perplexity: 3.103 speed: 168 wps\n",
      "100*Loss 119.181\n",
      "total_test_loss 1.0670640\n",
      "0.032 perplexity: 2.983 speed: 163 wps\n",
      "100*Loss 74.254\n",
      "total_test_loss 1.0457547\n",
      "0.002 perplexity: 2.944 speed: 159 wps\n",
      "100*Loss 100.602\n",
      "total_test_loss 1.0692357\n",
      "0.012 perplexity: 2.915 speed: 164 wps\n",
      "100*Loss 87.193\n",
      "total_test_loss 1.0462209\n",
      "0.022 perplexity: 2.930 speed: 171 wps\n",
      "100*Loss 122.483\n",
      "total_test_loss 1.0522829\n",
      "0.032 perplexity: 2.899 speed: 168 wps\n",
      "100*Loss 72.701\n",
      "total_test_loss 1.0427447\n",
      "0.002 perplexity: 2.890 speed: 165 wps\n",
      "100*Loss 100.530\n",
      "total_test_loss 1.0655729\n",
      "0.012 perplexity: 2.879 speed: 167 wps\n",
      "100*Loss 86.657\n",
      "total_test_loss 1.0446350\n",
      "0.022 perplexity: 2.891 speed: 171 wps\n",
      "100*Loss 122.468\n",
      "total_test_loss 1.0516896\n",
      "0.032 perplexity: 2.874 speed: 170 wps\n",
      "100*Loss 72.438\n",
      "total_test_loss 1.0416282\n",
      "0.002 perplexity: 2.870 speed: 168 wps\n",
      "100*Loss 100.580\n",
      "total_test_loss 1.0648550\n",
      "0.012 perplexity: 2.863 speed: 169 wps\n",
      "100*Loss 86.461\n",
      "total_test_loss 1.0434174\n",
      "0.022 perplexity: 2.873 speed: 172 wps\n",
      "100*Loss 121.952\n",
      "total_test_loss 1.0535896\n",
      "0.032 perplexity: 2.861 speed: 170 wps\n",
      "100*Loss 71.822\n",
      "total_test_loss 1.0399656\n",
      "0.002 perplexity: 2.858 speed: 169 wps\n",
      "100*Loss 100.204\n",
      "total_test_loss 1.0615167\n",
      "0.012 perplexity: 2.853 speed: 170 wps\n",
      "100*Loss 86.012\n",
      "total_test_loss 1.0415305\n",
      "0.022 perplexity: 2.861 speed: 172 wps\n",
      "100*Loss 121.000\n",
      "total_test_loss 1.0577463\n",
      "0.032 perplexity: 2.851 speed: 171 wps\n",
      "100*Loss 70.719\n",
      "total_test_loss 1.0367817\n",
      "0.002 perplexity: 2.848 speed: 169 wps\n",
      "100*Loss 100.155\n",
      "total_test_loss 1.0576560\n",
      "0.012 perplexity: 2.844 speed: 170 wps\n",
      "100*Loss 85.025\n",
      "total_test_loss 1.0409793\n",
      "0.022 perplexity: 2.850 speed: 172 wps\n",
      "100*Loss 119.819\n",
      "total_test_loss 1.0602206\n",
      "0.032 perplexity: 2.840 speed: 171 wps\n",
      "100*Loss 68.938\n",
      "total_test_loss 1.0320075\n",
      "0.002 perplexity: 2.836 speed: 170 wps\n",
      "100*Loss 98.514\n",
      "total_test_loss 1.0512567\n",
      "0.012 perplexity: 2.833 speed: 170 wps\n",
      "100*Loss 83.956\n",
      "total_test_loss 1.0367690\n",
      "0.022 perplexity: 2.837 speed: 172 wps\n",
      "100*Loss 118.416\n",
      "total_test_loss 1.0572678\n",
      "0.032 perplexity: 2.828 speed: 171 wps\n",
      "100*Loss 68.002\n",
      "total_test_loss 1.0227599\n",
      "0.002 perplexity: 2.824 speed: 170 wps\n",
      "100*Loss 99.320\n",
      "total_test_loss 1.0603145\n",
      "0.012 perplexity: 2.821 speed: 170 wps\n",
      "100*Loss 82.261\n",
      "total_test_loss 1.0372835\n",
      "0.022 perplexity: 2.824 speed: 172 wps\n",
      "100*Loss 116.354\n",
      "total_test_loss 1.0467526\n",
      "0.032 perplexity: 2.816 speed: 171 wps\n",
      "100*Loss 66.612\n",
      "total_test_loss 1.0206398\n",
      "0.002 perplexity: 2.811 speed: 170 wps\n",
      "100*Loss 98.700\n",
      "total_test_loss 1.0592547\n",
      "0.012 perplexity: 2.809 speed: 171 wps\n",
      "100*Loss 81.466\n",
      "total_test_loss 1.0405639\n",
      "0.022 perplexity: 2.810 speed: 172 wps\n",
      "100*Loss 115.958\n",
      "total_test_loss 1.0385575\n",
      "0.032 perplexity: 2.802 speed: 171 wps\n",
      "100*Loss 65.128\n",
      "total_test_loss 1.0162627\n",
      "0.002 perplexity: 2.799 speed: 171 wps\n",
      "100*Loss 97.619\n",
      "total_test_loss 1.0497094\n",
      "0.012 perplexity: 2.795 speed: 171 wps\n",
      "100*Loss 79.762\n",
      "total_test_loss 1.0414514\n",
      "0.022 perplexity: 2.796 speed: 172 wps\n",
      "100*Loss 118.348\n",
      "total_test_loss 1.0722080\n",
      "0.032 perplexity: 2.789 speed: 171 wps\n",
      "100*Loss 66.179\n",
      "total_test_loss 1.0057643\n",
      "0.002 perplexity: 2.783 speed: 171 wps\n",
      "100*Loss 95.741\n",
      "total_test_loss 1.0294982\n",
      "0.012 perplexity: 2.780 speed: 171 wps\n",
      "100*Loss 79.200\n",
      "total_test_loss 1.0248497\n",
      "0.022 perplexity: 2.780 speed: 172 wps\n",
      "100*Loss 115.817\n",
      "total_test_loss 1.0050340\n",
      "0.032 perplexity: 2.773 speed: 172 wps\n",
      "100*Loss 64.850\n",
      "total_test_loss 1.0011428\n",
      "0.002 perplexity: 2.770 speed: 171 wps\n",
      "100*Loss 95.698\n",
      "total_test_loss 1.0228594\n",
      "0.012 perplexity: 2.766 speed: 171 wps\n",
      "100*Loss 77.692\n",
      "total_test_loss 1.0255478\n",
      "0.022 perplexity: 2.765 speed: 172 wps\n",
      "100*Loss 110.675\n",
      "total_test_loss 0.9772444\n",
      "0.032 perplexity: 2.757 speed: 172 wps\n",
      "100*Loss 65.226\n",
      "total_test_loss 0.9976476\n",
      "0.002 perplexity: 2.752 speed: 171 wps\n",
      "100*Loss 95.097\n",
      "total_test_loss 1.0303714\n",
      "0.012 perplexity: 2.749 speed: 172 wps\n",
      "100*Loss 77.026\n",
      "total_test_loss 1.0152310\n",
      "0.022 perplexity: 2.748 speed: 172 wps\n",
      "100*Loss 101.694\n",
      "total_test_loss 0.9820419\n",
      "0.032 perplexity: 2.740 speed: 172 wps\n",
      "100*Loss 64.515\n",
      "total_test_loss 0.9837190\n",
      "0.002 perplexity: 2.734 speed: 171 wps\n",
      "100*Loss 93.839\n",
      "total_test_loss 1.0214374\n",
      "0.012 perplexity: 2.732 speed: 172 wps\n",
      "100*Loss 76.332\n",
      "total_test_loss 1.0133667\n",
      "0.022 perplexity: 2.730 speed: 173 wps\n",
      "100*Loss 99.253\n",
      "total_test_loss 0.9856694\n",
      "0.032 perplexity: 2.723 speed: 172 wps\n",
      "100*Loss 64.358\n",
      "total_test_loss 0.9776922\n",
      "0.002 perplexity: 2.720 speed: 172 wps\n",
      "100*Loss 98.181\n",
      "total_test_loss 1.0298444\n",
      "0.012 perplexity: 2.717 speed: 172 wps\n",
      "100*Loss 74.844\n",
      "total_test_loss 1.0013927\n",
      "0.022 perplexity: 2.714 speed: 173 wps\n",
      "100*Loss 93.746\n",
      "total_test_loss 0.9609313\n",
      "0.032 perplexity: 2.708 speed: 172 wps\n",
      "100*Loss 63.536\n",
      "total_test_loss 0.9669372\n",
      "0.002 perplexity: 2.704 speed: 172 wps\n",
      "100*Loss 96.702\n",
      "total_test_loss 1.0102109\n",
      "0.012 perplexity: 2.701 speed: 172 wps\n",
      "100*Loss 76.386\n",
      "total_test_loss 0.9491285\n",
      "0.022 perplexity: 2.698 speed: 173 wps\n",
      "100*Loss 89.960\n",
      "total_test_loss 0.9492464\n",
      "0.032 perplexity: 2.691 speed: 172 wps\n",
      "100*Loss 63.563\n",
      "total_test_loss 0.9461327\n",
      "0.002 perplexity: 2.688 speed: 172 wps\n",
      "100*Loss 95.833\n",
      "total_test_loss 0.9812850\n",
      "0.012 perplexity: 2.684 speed: 172 wps\n",
      "100*Loss 75.072\n",
      "total_test_loss 0.9207367\n",
      "0.022 perplexity: 2.681 speed: 173 wps\n",
      "100*Loss 98.436\n",
      "total_test_loss 0.9343369\n",
      "0.032 perplexity: 2.675 speed: 172 wps\n",
      "100*Loss 67.950\n",
      "total_test_loss 0.9250697\n",
      "0.002 perplexity: 2.670 speed: 172 wps\n",
      "100*Loss 89.515\n",
      "total_test_loss 0.9908498\n",
      "0.012 perplexity: 2.667 speed: 172 wps\n",
      "100*Loss 73.210\n",
      "total_test_loss 0.9184564\n",
      "0.022 perplexity: 2.663 speed: 173 wps\n",
      "100*Loss 101.849\n",
      "total_test_loss 0.9249194\n",
      "0.032 perplexity: 2.656 speed: 173 wps\n",
      "100*Loss 61.929\n",
      "total_test_loss 0.9133522\n",
      "0.002 perplexity: 2.653 speed: 172 wps\n",
      "100*Loss 94.166\n",
      "total_test_loss 0.9390885\n",
      "0.012 perplexity: 2.649 speed: 172 wps\n",
      "100*Loss 72.178\n",
      "total_test_loss 0.8926851\n",
      "0.022 perplexity: 2.645 speed: 173 wps\n",
      "100*Loss 94.580\n",
      "total_test_loss 0.9164693\n",
      "0.032 perplexity: 2.638 speed: 173 wps\n",
      "100*Loss 62.050\n",
      "total_test_loss 0.9059997\n",
      "0.002 perplexity: 2.634 speed: 172 wps\n",
      "100*Loss 91.935\n",
      "total_test_loss 0.9138177\n",
      "0.012 perplexity: 2.630 speed: 173 wps\n",
      "100*Loss 71.871\n",
      "total_test_loss 0.8726329\n",
      "0.022 perplexity: 2.626 speed: 173 wps\n",
      "100*Loss 84.323\n",
      "total_test_loss 0.8927240\n",
      "0.032 perplexity: 2.619 speed: 173 wps\n",
      "100*Loss 60.679\n",
      "total_test_loss 0.9004304\n",
      "0.002 perplexity: 2.615 speed: 172 wps\n",
      "100*Loss 85.767\n",
      "total_test_loss 0.9062948\n",
      "0.012 perplexity: 2.611 speed: 173 wps\n",
      "100*Loss 70.337\n",
      "total_test_loss 0.8541699\n",
      "0.022 perplexity: 2.606 speed: 173 wps\n",
      "100*Loss 80.397\n",
      "total_test_loss 0.8616459\n",
      "0.032 perplexity: 2.599 speed: 173 wps\n",
      "100*Loss 60.448\n",
      "total_test_loss 0.8877398\n",
      "0.002 perplexity: 2.597 speed: 172 wps\n",
      "100*Loss 95.892\n",
      "total_test_loss 0.9236806\n",
      "0.012 perplexity: 2.592 speed: 173 wps\n",
      "100*Loss 71.843\n",
      "total_test_loss 0.8471131\n",
      "0.022 perplexity: 2.587 speed: 173 wps\n",
      "100*Loss 76.164\n",
      "total_test_loss 0.8625980\n",
      "0.032 perplexity: 2.580 speed: 173 wps\n",
      "100*Loss 60.051\n",
      "total_test_loss 0.8749051\n",
      "0.002 perplexity: 2.577 speed: 173 wps\n",
      "100*Loss 88.331\n",
      "total_test_loss 0.9238483\n",
      "0.012 perplexity: 2.572 speed: 173 wps\n",
      "100*Loss 70.331\n",
      "total_test_loss 0.8527135\n",
      "0.022 perplexity: 2.567 speed: 173 wps\n",
      "100*Loss 77.421\n",
      "total_test_loss 0.8513373\n",
      "0.032 perplexity: 2.560 speed: 173 wps\n",
      "100*Loss 63.405\n",
      "total_test_loss 0.8486231\n",
      "0.002 perplexity: 2.557 speed: 173 wps\n",
      "100*Loss 85.066\n",
      "total_test_loss 0.9036893\n",
      "0.012 perplexity: 2.553 speed: 173 wps\n",
      "100*Loss 67.952\n",
      "total_test_loss 0.8520983\n",
      "0.022 perplexity: 2.547 speed: 173 wps\n",
      "100*Loss 71.989\n",
      "total_test_loss 0.8289587\n",
      "0.032 perplexity: 2.541 speed: 173 wps\n",
      "100*Loss 59.741\n",
      "total_test_loss 0.8526739\n",
      "0.002 perplexity: 2.538 speed: 173 wps\n",
      "100*Loss 97.116\n",
      "total_test_loss 0.8980828\n",
      "0.012 perplexity: 2.533 speed: 173 wps\n",
      "100*Loss 66.781\n",
      "total_test_loss 0.8436789\n",
      "0.022 perplexity: 2.528 speed: 173 wps\n",
      "100*Loss 71.132\n",
      "total_test_loss 0.8234092\n",
      "0.032 perplexity: 2.521 speed: 173 wps\n",
      "100*Loss 57.860\n",
      "total_test_loss 0.8660668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002 perplexity: 2.518 speed: 173 wps\n",
      "100*Loss 78.773\n",
      "total_test_loss 0.8527331\n",
      "0.012 perplexity: 2.513 speed: 173 wps\n",
      "100*Loss 65.548\n",
      "total_test_loss 0.8273471\n",
      "0.022 perplexity: 2.508 speed: 173 wps\n",
      "100*Loss 69.435\n",
      "total_test_loss 0.8105679\n",
      "0.032 perplexity: 2.502 speed: 173 wps\n",
      "100*Loss 56.106\n",
      "total_test_loss 0.8204950\n",
      "0.002 perplexity: 2.498 speed: 173 wps\n",
      "100*Loss 83.431\n",
      "total_test_loss 0.8506708\n",
      "0.012 perplexity: 2.493 speed: 173 wps\n",
      "100*Loss 64.831\n",
      "total_test_loss 0.8111011\n",
      "0.022 perplexity: 2.488 speed: 173 wps\n",
      "100*Loss 68.529\n",
      "total_test_loss 0.7988731\n",
      "0.032 perplexity: 2.482 speed: 173 wps\n",
      "100*Loss 57.890\n",
      "total_test_loss 0.8115598\n",
      "0.002 perplexity: 2.479 speed: 173 wps\n",
      "100*Loss 85.688\n",
      "total_test_loss 0.8398455\n",
      "0.012 perplexity: 2.474 speed: 173 wps\n",
      "100*Loss 61.773\n",
      "total_test_loss 0.8078284\n",
      "0.022 perplexity: 2.469 speed: 174 wps\n",
      "100*Loss 67.930\n",
      "total_test_loss 0.7970693\n",
      "0.032 perplexity: 2.463 speed: 173 wps\n",
      "100*Loss 56.185\n",
      "total_test_loss 0.7892687\n",
      "0.002 perplexity: 2.459 speed: 173 wps\n",
      "100*Loss 98.436\n",
      "total_test_loss 0.9334104\n",
      "0.012 perplexity: 2.455 speed: 173 wps\n",
      "100*Loss 63.515\n",
      "total_test_loss 0.8078119\n",
      "0.022 perplexity: 2.450 speed: 174 wps\n",
      "100*Loss 64.754\n",
      "total_test_loss 0.7846086\n",
      "0.032 perplexity: 2.444 speed: 173 wps\n",
      "100*Loss 58.485\n",
      "total_test_loss 0.7621370\n",
      "0.002 perplexity: 2.441 speed: 173 wps\n",
      "100*Loss 92.334\n",
      "total_test_loss 0.8153000\n",
      "0.012 perplexity: 2.436 speed: 173 wps\n",
      "100*Loss 59.084\n",
      "total_test_loss 0.7827309\n",
      "0.022 perplexity: 2.431 speed: 174 wps\n",
      "100*Loss 62.036\n",
      "total_test_loss 0.7809432\n",
      "0.032 perplexity: 2.426 speed: 174 wps\n",
      "100*Loss 59.030\n",
      "total_test_loss 0.7702905\n",
      "0.002 perplexity: 2.423 speed: 173 wps\n",
      "100*Loss 87.942\n",
      "total_test_loss 0.8095887\n",
      "0.012 perplexity: 2.418 speed: 173 wps\n",
      "100*Loss 60.411\n",
      "total_test_loss 0.7829151\n",
      "0.022 perplexity: 2.413 speed: 174 wps\n",
      "100*Loss 59.111\n",
      "total_test_loss 0.7657427\n",
      "0.032 perplexity: 2.408 speed: 174 wps\n",
      "100*Loss 56.506\n",
      "total_test_loss 0.7604417\n",
      "0.002 perplexity: 2.404 speed: 173 wps\n",
      "100*Loss 84.012\n",
      "total_test_loss 0.8076744\n",
      "0.012 perplexity: 2.400 speed: 174 wps\n",
      "100*Loss 57.977\n",
      "total_test_loss 0.7897314\n",
      "0.022 perplexity: 2.395 speed: 174 wps\n",
      "100*Loss 55.970\n",
      "total_test_loss 0.7643788\n",
      "0.032 perplexity: 2.389 speed: 174 wps\n",
      "100*Loss 56.254\n",
      "total_test_loss 0.7589896\n",
      "0.002 perplexity: 2.386 speed: 174 wps\n",
      "100*Loss 88.792\n",
      "total_test_loss 0.8029257\n",
      "0.012 perplexity: 2.382 speed: 174 wps\n",
      "100*Loss 59.710\n",
      "total_test_loss 0.7745458\n",
      "0.022 perplexity: 2.377 speed: 174 wps\n",
      "100*Loss 55.228\n",
      "total_test_loss 0.7699542\n",
      "0.032 perplexity: 2.372 speed: 174 wps\n",
      "100*Loss 56.772\n",
      "total_test_loss 0.7716715\n",
      "0.002 perplexity: 2.368 speed: 174 wps\n",
      "100*Loss 89.057\n",
      "total_test_loss 0.8172187\n",
      "0.012 perplexity: 2.364 speed: 174 wps\n",
      "100*Loss 56.273\n",
      "total_test_loss 0.7847945\n",
      "0.022 perplexity: 2.359 speed: 174 wps\n",
      "100*Loss 53.757\n",
      "total_test_loss 0.7644235\n",
      "0.032 perplexity: 2.354 speed: 174 wps\n",
      "100*Loss 55.994\n",
      "total_test_loss 0.7702076\n",
      "0.002 perplexity: 2.350 speed: 174 wps\n",
      "100*Loss 89.346\n",
      "total_test_loss 0.8063263\n",
      "0.012 perplexity: 2.346 speed: 174 wps\n",
      "100*Loss 61.239\n",
      "total_test_loss 0.7936607\n",
      "0.022 perplexity: 2.341 speed: 174 wps\n",
      "100*Loss 51.473\n",
      "total_test_loss 0.7600632\n",
      "0.032 perplexity: 2.336 speed: 174 wps\n",
      "100*Loss 51.013\n",
      "total_test_loss 0.7539684\n",
      "0.002 perplexity: 2.333 speed: 174 wps\n",
      "100*Loss 83.762\n",
      "total_test_loss 0.8108298\n",
      "0.012 perplexity: 2.329 speed: 174 wps\n",
      "100*Loss 56.868\n",
      "total_test_loss 0.7971184\n",
      "0.022 perplexity: 2.324 speed: 174 wps\n",
      "100*Loss 49.154\n",
      "total_test_loss 0.7624342\n",
      "0.032 perplexity: 2.319 speed: 174 wps\n",
      "100*Loss 50.994\n",
      "total_test_loss 0.7457816\n",
      "0.002 perplexity: 2.316 speed: 174 wps\n",
      "100*Loss 92.945\n",
      "total_test_loss 0.8232199\n",
      "0.012 perplexity: 2.312 speed: 174 wps\n",
      "100*Loss 57.791\n",
      "total_test_loss 0.8387309\n",
      "0.022 perplexity: 2.307 speed: 174 wps\n",
      "100*Loss 49.541\n",
      "total_test_loss 0.7403467\n",
      "0.032 perplexity: 2.303 speed: 174 wps\n",
      "100*Loss 54.218\n",
      "total_test_loss 0.7456142\n",
      "0.002 perplexity: 2.300 speed: 174 wps\n",
      "100*Loss 84.411\n",
      "total_test_loss 0.7943755\n",
      "0.012 perplexity: 2.296 speed: 174 wps\n",
      "100*Loss 56.185\n",
      "total_test_loss 0.8139123\n",
      "0.022 perplexity: 2.291 speed: 174 wps\n",
      "100*Loss 46.995\n",
      "total_test_loss 0.7388526\n",
      "0.032 perplexity: 2.287 speed: 174 wps\n",
      "100*Loss 54.473\n",
      "total_test_loss 0.7569524\n",
      "0.002 perplexity: 2.283 speed: 174 wps\n",
      "100*Loss 86.761\n",
      "total_test_loss 0.8432562\n",
      "0.012 perplexity: 2.279 speed: 174 wps\n",
      "100*Loss 59.071\n",
      "total_test_loss 0.8068442\n",
      "0.022 perplexity: 2.275 speed: 174 wps\n",
      "100*Loss 45.584\n",
      "total_test_loss 0.7278628\n",
      "0.032 perplexity: 2.270 speed: 174 wps\n",
      "100*Loss 53.297\n",
      "total_test_loss 0.7478987\n",
      "0.002 perplexity: 2.267 speed: 174 wps\n",
      "100*Loss 75.386\n",
      "total_test_loss 0.7794481\n",
      "0.012 perplexity: 2.263 speed: 174 wps\n",
      "100*Loss 56.492\n",
      "total_test_loss 0.8574400\n",
      "0.022 perplexity: 2.258 speed: 174 wps\n",
      "100*Loss 44.743\n",
      "total_test_loss 0.7429498\n",
      "0.032 perplexity: 2.254 speed: 174 wps\n",
      "100*Loss 53.935\n",
      "total_test_loss 0.7673567\n",
      "0.002 perplexity: 2.251 speed: 174 wps\n",
      "100*Loss 79.177\n",
      "total_test_loss 0.7800243\n",
      "0.012 perplexity: 2.247 speed: 174 wps\n",
      "100*Loss 48.847\n",
      "total_test_loss 0.7912501\n",
      "0.022 perplexity: 2.242 speed: 174 wps\n",
      "100*Loss 46.127\n",
      "total_test_loss 0.7426762\n",
      "0.032 perplexity: 2.238 speed: 174 wps\n",
      "100*Loss 54.256\n",
      "total_test_loss 0.7414102\n",
      "0.002 perplexity: 2.235 speed: 174 wps\n",
      "100*Loss 78.758\n",
      "total_test_loss 0.7396919\n",
      "0.012 perplexity: 2.231 speed: 174 wps\n",
      "100*Loss 53.081\n",
      "total_test_loss 0.8440193\n",
      "0.022 perplexity: 2.226 speed: 174 wps\n",
      "100*Loss 44.826\n",
      "total_test_loss 0.7672111\n",
      "0.032 perplexity: 2.222 speed: 174 wps\n",
      "100*Loss 49.037\n",
      "total_test_loss 0.7895808\n",
      "0.002 perplexity: 2.219 speed: 174 wps\n",
      "100*Loss 73.672\n",
      "total_test_loss 0.7340677\n",
      "0.012 perplexity: 2.215 speed: 174 wps\n",
      "100*Loss 51.152\n",
      "total_test_loss 0.8862173\n",
      "0.022 perplexity: 2.211 speed: 175 wps\n",
      "100*Loss 45.296\n",
      "total_test_loss 0.7335002\n",
      "0.032 perplexity: 2.207 speed: 174 wps\n",
      "100*Loss 47.410\n",
      "total_test_loss 0.7731464\n",
      "0.002 perplexity: 2.203 speed: 174 wps\n",
      "100*Loss 81.124\n",
      "total_test_loss 0.7719259\n",
      "0.012 perplexity: 2.199 speed: 174 wps\n",
      "100*Loss 46.351\n",
      "total_test_loss 0.8306298\n",
      "0.022 perplexity: 2.195 speed: 175 wps\n",
      "100*Loss 44.409\n",
      "total_test_loss 0.7613848\n",
      "0.032 perplexity: 2.191 speed: 174 wps\n",
      "100*Loss 60.910\n",
      "total_test_loss 0.7750053\n",
      "0.002 perplexity: 2.188 speed: 174 wps\n",
      "100*Loss 69.606\n",
      "total_test_loss 0.7764462\n",
      "0.012 perplexity: 2.184 speed: 174 wps\n",
      "100*Loss 48.575\n",
      "total_test_loss 0.8037510\n",
      "0.022 perplexity: 2.180 speed: 175 wps\n",
      "100*Loss 42.642\n",
      "total_test_loss 0.7605088\n",
      "0.032 perplexity: 2.177 speed: 174 wps\n",
      "100*Loss 49.453\n",
      "total_test_loss 0.7612857\n",
      "0.002 perplexity: 2.173 speed: 174 wps\n",
      "100*Loss 80.726\n",
      "total_test_loss 0.7629958\n",
      "0.012 perplexity: 2.169 speed: 174 wps\n",
      "100*Loss 51.575\n",
      "total_test_loss 0.9197867\n",
      "0.022 perplexity: 2.165 speed: 175 wps\n",
      "100*Loss 39.452\n",
      "total_test_loss 0.7507343\n",
      "0.032 perplexity: 2.162 speed: 175 wps\n",
      "100*Loss 47.210\n",
      "total_test_loss 0.7886577\n",
      "0.002 perplexity: 2.158 speed: 174 wps\n",
      "100*Loss 70.715\n",
      "total_test_loss 0.7690589\n",
      "0.012 perplexity: 2.154 speed: 174 wps\n",
      "100*Loss 42.361\n",
      "total_test_loss 0.8336695\n",
      "0.022 perplexity: 2.151 speed: 175 wps\n",
      "100*Loss 40.422\n",
      "total_test_loss 0.7456667\n",
      "0.032 perplexity: 2.147 speed: 175 wps\n",
      "100*Loss 46.084\n",
      "total_test_loss 0.7650304\n",
      "0.002 perplexity: 2.144 speed: 174 wps\n",
      "100*Loss 67.594\n",
      "total_test_loss 0.7339886\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for cycles in range(10000):\n",
    "    for step in range(epoch_size//batch_size):\n",
    "        feed_dict = {sentence_inputs_A:np.array(batchesA[step]), sentence_inputs_A_length:np.array(seq_lenA[step]),\n",
    "                     serial_index_offsets_A:create_index_offsets(len(batchesA[step]), sequence_len, seq_lenA[step]),\n",
    "                     sentence_inputs_B:np.array(batchesB[step]), sentence_inputs_B_length:np.array(seq_lenB[step]),\n",
    "                     serial_index_offsets_B:create_index_offsets(len(batchesB[step]), sequence_len, seq_lenB[step]),\n",
    "                     target_score:np.array(convert_scores_to_p(scores[step])), target_score_scalar:np.array(scores[step])}\n",
    "\n",
    "        fetches = {'loss': loss, 'train_op':train_op}\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"loss\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters +=  1\n",
    "\n",
    "        if (cycles == 0 and step == 0 ) or (step % (epoch_size // 100) == 10):\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                    (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                    iters * batch_size * max(1, 1) /\n",
    "                    (time.time() - start_time)))\n",
    "            print(\"100*Loss %.3f\" % (100*cost))\n",
    "            \n",
    "            global_step = cycles*epoch_size//batch_size+step\n",
    "            TrainLoss_summary_val = session.run(TrainLoss_summary, feed_dict)\n",
    "            writer.add_summary(TrainLoss_summary_val, global_step)\n",
    "            \n",
    "            total_test_loss, TestLoss_summary_val, MSE_summary_val, pMSE_summary_val = load_tree_test_data_and_test_model(test_batchesA, test_seq_lenA,test_batchesB, \n",
    "                                                                   test_seq_lenB,test_scores)\n",
    "            writer.add_summary(TestLoss_summary_val, global_step)\n",
    "            writer.add_summary(MSE_summary_val, global_step)\n",
    "            writer.add_summary(pMSE_summary_val, global_step)\n",
    "            \n",
    "            print(\"total_test_loss %.7f\" % (total_test_loss))\n",
    "            save_path = saver.save(session, \"./event_and_checkpoints/SemanticRelatednessLSTM-h.ckpt\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_batches_A = None\n",
    "sentence_batches_B = None\n",
    "sentence_len_A = None\n",
    "sentence_len_B = None\n",
    "test_scores = None \n",
    "\n",
    "def load_sentence_test_data_and_test_model(sentence_treesA, sentence_treesB, sentenceA_file_name, sentenceB_file_name, \n",
    "                                  score_file_name, output_file_name):\n",
    "    if sentence_batches_A:\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "               output_file_name)\n",
    "    else:\n",
    "        \n",
    "        if not sentenceA_file_name:\n",
    "            sentenceA_file_name = 'data/sick_trial_sentenceA.txt'\n",
    "        if not sentenceB_file_name:\n",
    "            sentenceB_file_name = 'data/sick_trial_sentenceB.txt'\n",
    "        if not score_file_name:\n",
    "            score_file_name = 'data/sick_trial_score.txt'\n",
    "        if not output_file_name:\n",
    "            output_file_name = \"./data/sick_trial_score_sentence_predict.csv\" \n",
    "\n",
    "        sentencesA = load_lines(sentenceA_file_name)\n",
    "        sentencesB = load_lines(sentenceB_file_name)\n",
    "\n",
    "        sentence_batches_A, max_sentence_lengthA, sentence_len_A = create_sentence_batches(sentencesA, sentence_treesA, batch_size)\n",
    "        sentence_batches_B, max_sentence_lengthB, sentence_len_B = create_sentence_batches(sentencesB, sentence_treesB, batch_size)\n",
    "\n",
    "        pad_sequences(sentence_batches_A, sequence_len)\n",
    "        pad_sequences(sentence_batches_B, sequence_len)\n",
    "\n",
    "        test_scores = load_scores(score_file_name, batch_size)\n",
    "\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "                   output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
