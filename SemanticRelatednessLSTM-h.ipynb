{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import external_lib as el\n",
    "from itertools import permutations\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_glove_path, = el.download_and_unzip(\n",
    "#  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "#  'glove.840B.300d.txt', data_dir = \"./data_sources/glove.6B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'data/sick_filtered_glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove_path = 'data_sources/glove.6B/glove.840B.300d.txt'\n",
    "#el.filter_glove(full_glove_path, filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from data/sick_filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = el.load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])\n",
    "        \n",
    "    def copy(self, parent=None):\n",
    "        root = Node(data.copy())\n",
    "        if parent: \n",
    "            root.add_parent(parent)\n",
    "        for child in self.children:\n",
    "            root.add_child(child.copy(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate copies of the tree with diffrent arrangement orders of children for nodes in generation_to_rearrange\"\"\"\n",
    "\n",
    "def generate_tree_permutations(tree, generation_to_rearrange=1):\n",
    "    target_generation, target_generation_addresses = get_generation_and_addresses(tree, generation_to_rearrange=1)\n",
    "    tree_perms = []\n",
    "    if target_generation: \n",
    "        for i, parent in enumerate(target_generation):\n",
    "            if len(parent.children) > 1:\n",
    "                perms = get_permutations_for(len(parent.children))\n",
    "                for j, perm in enumerate(perms):\n",
    "                    #make a copy of the tree\n",
    "                    #rearrange the children of parent using the func below \n",
    "                    #add the result to tree_perms\n",
    "                    print(\"TODO\")\n",
    "\n",
    "def get_permutations_for(size):\n",
    "    indexes = list(range(size))\n",
    "    return list(permutations(indexes))\n",
    "    \n",
    "\"\"\"Returns None when there are no possible permutatations in the selected generation\"\"\"\n",
    "def get_generation_and_addresses(tree, generation_to_rearrange=1):\n",
    "    target_generation_addresses = None\n",
    "    target_generation = None\n",
    "    if len(tree.children) < 2 and generation_to_rearrange==0:\n",
    "        return target_generation, target_generation_addresses\n",
    "    elif generation_to_rearrange == 0:\n",
    "        return [tree], [[0]]\n",
    "    \n",
    "    prev_gen = [tree]\n",
    "    prev_gen_addresses = [[0]]\n",
    "    \n",
    "    next_gen = []\n",
    "    next_gen_addresses = []\n",
    "    current_gen = 0\n",
    "    while len(prev_gen)>0:\n",
    "        next_gen = []\n",
    "        next_gen_addresses = []\n",
    "        for i, parent in enumerate(prev_gen):\n",
    "            if len(parent.children)>1:\n",
    "                for j, child in enumerate(parent.children):\n",
    "                    next_gen.append(child)\n",
    "                    parent_address = prev_gen_addresses[i].copy()\n",
    "                    parent_address.append(j)\n",
    "                    next_gen_addresses.append(parent_address)\n",
    "        \n",
    "        current_gen += 1\n",
    "        if current_gen == generation_to_rearrange:\n",
    "            target_generation = next_gen\n",
    "            target_generation_addresses = next_gen_addresses\n",
    "            break\n",
    "        prev_gen = next_gen\n",
    "        prev_gen_addresses = next_gen_addresses\n",
    "        \n",
    "    return target_generation, target_generation_addresses\n",
    "    \n",
    "def rearrange_list(list_, index_order):\n",
    "    list_ = [ list_[i] for i in index_order]\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\n",
    "Parse indented lines of text into a tree. Children are indented & under the parent\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert trees to a linear representation. Children are listed between the left and right \n",
    "marker in front of the parent. Each word is replaced by its id \"\"\"\n",
    "unknown_word = word_to_idx[\"UNKNOWN_WORD\"]\n",
    "left_marker = word_to_idx[\"LEFT_MARKER\"]\n",
    "right_marker = word_to_idx[\"RIGHT_MARKER\"]\n",
    "end_marker = word_to_idx[\"END_MARKER\"]\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        tree_batches.append(tree_batch)\n",
    "        batch = []\n",
    "        batches.append(batch)\n",
    "        batch_lengths = []\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for tree in tree_batch:\n",
    "            result =[]\n",
    "            batch.append(result)\n",
    "            handle_node(tree, result)\n",
    "            batch_lengths.append(len(result))\n",
    "            if len(result) > max_sequence_length:\n",
    "                max_sequence_length = len(result)\n",
    "    \n",
    "    return batches, tree_batches, max_sequence_length,batches_lengths\n",
    "                \n",
    "            \n",
    "def handle_node(node, result):\n",
    "    result.append(left_marker)\n",
    "    word = node.data[0]\n",
    "    if word in word_to_idx:\n",
    "        result.append(word_to_idx[word])\n",
    "    else:\n",
    "        result.append(unknown_word)\n",
    "        #print(\"Unknown word: \"+word)\n",
    "    if len(node.children)>0:\n",
    "        \n",
    "        for child in node.children:\n",
    "            handle_node(child, result)\n",
    "    result.append(right_marker)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pad sequences with end markers\"\"\"\n",
    "def pad_sequences(batches, max_sequence_length):\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length :\n",
    "                sentence.append(end_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert the score into a set of probabilities over the classes\"\"\"\n",
    "\"\"\"Since the loss uses KL Divergence the PMF cannot have 0 values \"\"\"\n",
    "def convert_scores_to_p(scores_list):\n",
    "    scores = np.array(scores_list) \n",
    "    num_of_classes = 5 #1, 2, .. , 4, 5\n",
    "    p = np.zeros((len(scores), num_of_classes))\n",
    "    for i, score in enumerate(scores): \n",
    "        floor = math.floor(score)\n",
    "        if score == num_of_classes:\n",
    "            p[i] = p[i]+0.0001\n",
    "            p[i][num_of_classes-1] = 0.9996\n",
    "        elif floor == score:\n",
    "            p[i] = p[i] + 0.0001\n",
    "            p[i][floor-1] = 0.9996\n",
    "        else:\n",
    "            p[i] = p[i] + 0.0001\n",
    "            p[i][floor] = score - floor - 0.00015 #floor+1-1  zero index adjustment\n",
    "            p[i][floor-1] = floor - score + 1 - 0.00015 #floor-1  zero index adjustment\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the sentences into words and convert the words to their ids\n",
    "The set of words by which to split the sentence can be found in the corresponding tree\n",
    "so fetch the set of words first \"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "def create_sentence_batches(sentences, trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        sentence_batch = sentences[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        batch = []\n",
    "        batch_lengths = []\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for j, tree in enumerate(tree_batch):\n",
    "            word_list =[]\n",
    "            get_word_list(tree, word_list)\n",
    "            #set_trace()\n",
    "            sentence_ids = []\n",
    "            batch.append(sentence_ids)\n",
    "            ordered_word_list = sentence_batch[j].replace(\",\", \" , \").replace(\".\", \" . \").replace(\"n't\", \" n't\").replace(\"'s\", \" 's \").split()\n",
    "            \n",
    "            for k in range(len(ordered_word_list)):\n",
    "                word = ordered_word_list[k]\n",
    "                if not word in word_list:\n",
    "                    print(\"missing word: \" + word)\n",
    "                    set_trace()\n",
    "                    for token in word_list:\n",
    "                        if (not token in ordered_word_list) and token in word:\n",
    "                            words = word.replace(token, \" \"+token+\" \").split()\n",
    "                            for half_word in words:\n",
    "                                if half_word in word_to_idx:\n",
    "                                    sentence_ids.append(word_to_idx[half_word])\n",
    "                                else:\n",
    "                                    sentence_ids.append(unknown_word)\n",
    "                            break\n",
    "                elif word in word_to_idx:\n",
    "                    sentence_ids.append(word_to_idx[word])\n",
    "                else:\n",
    "                    sentence_ids.append(unknown_word)\n",
    "            batch_lengths.append(len(sentence_ids))\n",
    "            if len(sentence_ids) > max_sequence_length:\n",
    "                max_sequence_length = len(sentence_ids)\n",
    "    \n",
    "    return batches, max_sequence_length, batches_lengths\n",
    "                \n",
    "            \n",
    "def get_word_list(node, word_list):\n",
    "    word = node.data[0]\n",
    "    word_list.append(word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            get_word_list(child, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(file):\n",
    "    with open(file, 'r') as f: \n",
    "        contents = f.readlines()\n",
    "        if len(contents[-1].strip())==0:\n",
    "            contents.pop(-1)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_offsets(batch_size, sequence_len, input_lengths):\n",
    "    return np.array(range(batch_size))*sequence_len + input_lengths-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "sequence_len = 100\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 5 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10\n",
    "learn_rate = 0.05\n",
    "max_grad_norm = 5\n",
    "epoch_size = 6000\n",
    "min_test_loss = 1000 #initialize to high value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_train_sentenceA_tree.txt')\n",
    "sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_train_sentenceB_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesA, tree_batchesA, max_sequence_lengthA, seq_lenA = create_batches(sentence_treesA, batch_size)\n",
    "batchesB, tree_batchesB, max_sequence_lengthB, seq_lenB = create_batches(sentence_treesB, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = max(max_sequence_lengthA, max_sequence_lengthB)\n",
    "sequence_len_tensor = tf.constant(sequence_len, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(batchesA, sequence_len)\n",
    "pad_sequences(batchesB, sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores('data/sick_train_score.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "sentence_inputs_A = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_A\")\n",
    "sentence_inputs_A_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_A_length\")\n",
    "serial_index_offsets_A = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_A\")\n",
    "\n",
    "sentence_inputs_B = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_B\")\n",
    "sentence_inputs_B_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_B_length\")\n",
    "serial_index_offsets_B = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_B\")\n",
    "\n",
    "target_score = tf.placeholder(data_type, shape=(None, output_size), name=\"target_scores\")\n",
    "target_score_scalar = tf.placeholder(data_type, shape=(None, ), name=\"target_scores\")\n",
    "\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "embedded_inputs_A = tf.nn.embedding_lookup(embedding, sentence_inputs_A)\n",
    "embedded_inputs_B = tf.nn.embedding_lookup(embedding, sentence_inputs_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def makeCells():\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        cell1 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        cell2 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    return [cell1, cell2]\n",
    "\n",
    "A =makeCells()\n",
    "B =makeCells()\n",
    "with tf.variable_scope(\"UnrolledStackedCells\", reuse=tf.AUTO_REUSE):\n",
    "    cellA = tf.contrib.rnn.MultiRNNCell(A, state_is_tuple=True)\n",
    "    outputsA, final_stateA = tf.nn.dynamic_rnn(cellA, embedded_inputs_A,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_A_length)\n",
    "    \n",
    "    cellB = tf.contrib.rnn.MultiRNNCell(B, state_is_tuple=True)\n",
    "    outputsB, final_stateB = tf.nn.dynamic_rnn(cellB, embedded_inputs_B,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_B_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'UnrolledStackedCells/rnn/transpose_1:0' shape=(?, 108, 300) dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_outputsA = tf.reshape(outputsA, [-1, hidden_size])\n",
    "terminal_outputsA = tf.gather(serialized_outputsA, serial_index_offsets_A)\n",
    "\n",
    "serialized_outputsB = tf.reshape(outputsB, [-1, hidden_size])\n",
    "terminal_outputsB = tf.gather(serialized_outputsB, serial_index_offsets_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.abs(tf.subtract(terminal_outputsA, terminal_outputsB))\n",
    "h_elewise_product = tf.multiply(terminal_outputsA, terminal_outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) \n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = tf.multiply(p_hat, tf.constant([1,2,3,4,5], dtype=tf.float64))\n",
    "y_hat = tf.reduce_sum(y_p, 1)\n",
    "MSE = tf.losses.mean_squared_error(target_score_scalar, y_hat)\n",
    "pMSE = tf.cast(tf.losses.mean_squared_error(target_score, p_hat), data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "p_over_p_hat = tf.div(target_score, p_hat)\n",
    "log_p_over_p_hat = tf.log(p_over_p_hat)\n",
    "KL = tf.reduce_mean(tf.reduce_sum(tf.multiply(target_score, log_p_over_p_hat), 1))\n",
    "regularizer = tf.constant(0.0,dtype=data_type)\n",
    "for var in tf.trainable_variables(): \n",
    "    regularizer = tf.add(regularizer, tf.nn.l2_loss(var))\n",
    "loss = KL + reg_lambda*regularizer\n",
    "\n",
    "learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),\n",
    "                                      max_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "TrainLoss_summary = tf.summary.scalar('TrainLoss', loss)\n",
    "TestLoss_summary = tf.summary.scalar('TestLoss', loss)\n",
    "MSE_summary = tf.summary.scalar('Test_MSE', MSE)\n",
    "pMSE_summary = tf.summary.scalar('Test_pMSE', pMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(batchesA, batchesB, seq_lenA, seq_lenB, scores, output_file):\n",
    "    global min_test_loss, saver\n",
    "    output_scores=[]\n",
    "    total_loss = 0\n",
    "    feed_dict = {sentence_inputs_A:np.array(batchesA), sentence_inputs_A_length:np.array(seq_lenA), \n",
    "                 serial_index_offsets_A:create_index_offsets(len(batchesA), sequence_len, seq_lenA),\n",
    "                 sentence_inputs_B:np.array(batchesB), sentence_inputs_B_length:np.array(seq_lenB),\n",
    "                 serial_index_offsets_B:create_index_offsets(len(batchesB), sequence_len, seq_lenB),\n",
    "                 target_score:np.array(convert_scores_to_p(scores)), target_score_scalar:np.array(scores) }\n",
    "\n",
    "\n",
    "    fetches = {'loss': loss, 'y_hat': y_hat, 'mse':MSE, 'TestLoss_summary':TestLoss_summary,\n",
    "              'MSE_summary':MSE_summary, 'pMSE_summary':pMSE_summary}\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    total_loss = vals[\"loss\"]\n",
    "    output_scores.append(vals[\"y_hat\"])\n",
    "\n",
    "\n",
    "    #print(\"Loss %.3f\" % (loss))\n",
    "    if vals[\"mse\"] < min_test_loss: \n",
    "        min_test_loss = vals[\"mse\"]\n",
    "        save_path = saver.save(session, \"./event_and_checkpoints/SemanticRelatednessLSTM-h_lowest_error.ckpt\")\n",
    "\n",
    "        with open(output_file,'w') as resultFile:\n",
    "            wr = csv.writer(resultFile, dialect='excel')\n",
    "            for batch in output_scores:\n",
    "                for score in batch: \n",
    "                    wr.writerow([repr(score)])\n",
    "    return total_loss, vals[\"TestLoss_summary\"], vals[\"MSE_summary\"], vals[\"pMSE_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchesA, test_seq_lenA = None, None\n",
    "test_batchesB, test_seq_lenB = None, None\n",
    "test_scores = None\n",
    "def load_tree_test_data_and_test_model(test_batchesA_, test_seq_lenA_,test_batchesB_, test_seq_lenB_,test_scores_):\n",
    "    global test_batchesA, test_seq_lenA, test_batchesB, test_seq_lenB, test_scores\n",
    "    if test_batchesA_ : \n",
    "        return test_model(test_batchesA_[0], test_batchesB_[0], test_seq_lenA_[0], test_seq_lenB_[0], test_scores_[0], \n",
    "                   \"test_results.txt\")\n",
    "    else:        \n",
    "        sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_trial_sentenceA_tree.txt')\n",
    "        sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_trial_sentenceB_tree.txt')\n",
    "\n",
    "        test_batchesA, test_tree_batchesA, max_sequence_lengthA, test_seq_lenA = create_batches(sentence_treesA,\n",
    "                                                                                                len(sentence_treesA))\n",
    "        test_batchesB, test_tree_batchesB, max_sequence_lengthB, test_seq_lenB = create_batches(sentence_treesB,\n",
    "                                                                                                len(sentence_treesB))\n",
    "\n",
    "        pad_sequences(test_batchesA, sequence_len)\n",
    "        pad_sequences(test_batchesB, sequence_len)\n",
    "\n",
    "        test_scores = load_scores('data/sick_trial_score.txt', len(sentence_treesA))\n",
    "        return test_model(test_batchesA[0], test_batchesB[0], test_seq_lenA[0], test_seq_lenB[0], test_scores[0], \n",
    "                       \"test_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_time = time.time()\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(session, \"./h_tree_KL_train_results/SemanticRelatednessLSTM-h.ckpt\")\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./event_and_checkpoints\", session.graph)\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_step(np_batchA, np_batchA_len, np_batchA_offsets, np_batchB, np_batchB_len, \n",
    "                      np_batchB_offsets, np_target_score, np_target_score_scalar):\n",
    "    global loss, train_op, session\n",
    "    feed_dict = {sentence_inputs_A:np_batchA, sentence_inputs_A_length:np_batchA_len,\n",
    "                 serial_index_offsets_A:np_batchA_offsets,\n",
    "                 sentence_inputs_B:np_batchB, sentence_inputs_B_length:np_batchB_len,\n",
    "                 serial_index_offsets_B:np_batchB_offsets,\n",
    "                 target_score:np_target_score, target_score_scalar:np_target_score_scalar}\n",
    "\n",
    "    fetches = {'loss': loss, 'train_op':train_op}\n",
    "    return session.run(fetches, feed_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 perplexity: 9.195 speed: 10 wps\n",
      "100*Loss 106.566\n",
      "total_test_loss 1.1441903\n",
      "0.042 perplexity: 8.092 speed: 4 wps\n",
      "100*Loss 100.368\n",
      "total_test_loss 1.0845803\n",
      "0.292 perplexity: 7.657 speed: 6 wps\n",
      "100*Loss 99.333\n",
      "total_test_loss 1.0424506\n",
      "0.542 perplexity: 8.233 speed: 6 wps\n",
      "100*Loss 81.960\n",
      "total_test_loss 1.0454939\n",
      "0.792 perplexity: 7.969 speed: 6 wps\n",
      "100*Loss 92.592\n",
      "total_test_loss 1.0566339\n",
      "0.042 perplexity: 7.900 speed: 6 wps\n",
      "100*Loss 97.922\n",
      "total_test_loss 1.0552585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-66050fa3f20f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#Switch sentence A and B then run the training step again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         vals = run_training_step(np_batchB, np_batchB_len, np_batchB_offsets, np_batchA, np_batchA_len, \n\u001b[0;32m---> 22\u001b[0;31m                       np_batchA_offsets, np_target_score, np_target_score_scalar)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-934cc27c961d>\u001b[0m in \u001b[0;36mrun_training_step\u001b[0;34m(np_batchA, np_batchA_len, np_batchA_offsets, np_batchB, np_batchB_len, np_batchB_offsets, np_target_score, np_target_score_scalar)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_op'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for cycles in range(10000):\n",
    "    for step in range(epoch_size//batch_size):\n",
    "        np_batchA = np.array(batchesA[step])\n",
    "        np_batchA_len = np.array(seq_lenA[step])\n",
    "        np_batchA_offsets = create_index_offsets(len(batchesA[step]), sequence_len, seq_lenA[step])\n",
    "        np_batchB = np.array(batchesB[step])\n",
    "        np_batchB_len = np.array(seq_lenB[step])\n",
    "        np_batchB_offsets = create_index_offsets(len(batchesB[step]), sequence_len, seq_lenB[step])\n",
    "        np_target_score = np.array(convert_scores_to_p(scores[step]))\n",
    "        np_target_score_scalar = np.array(scores[step])\n",
    "        \n",
    "        \n",
    "        vals = run_training_step(np_batchA, np_batchA_len, np_batchA_offsets, np_batchB, np_batchB_len, \n",
    "                      np_batchB_offsets, np_target_score, np_target_score_scalar)\n",
    "        cost = vals[\"loss\"]\n",
    "        costs += cost\n",
    "        \n",
    "        iters +=  1\n",
    "        \n",
    "        if (cycles == 0 and step == 0 ) or (step % (epoch_size // 100) == 10):\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                    (step * 1.0 / (epoch_size//batch_size), np.exp(costs / iters),\n",
    "                    iters * batch_size * max(1, 1) /\n",
    "                    (time.time() - start_time)))\n",
    "            print(\"100*Loss %.3f\" % (100*cost))\n",
    "            feed_dict = {sentence_inputs_A:np_batchA, sentence_inputs_A_length:np_batchA_len,\n",
    "                 serial_index_offsets_A:np_batchA_offsets,\n",
    "                 sentence_inputs_B:np_batchB, sentence_inputs_B_length:np_batchB_len,\n",
    "                 serial_index_offsets_B:np_batchB_offsets,\n",
    "                 target_score:np_target_score, target_score_scalar:np_target_score_scalar}\n",
    "            \n",
    "            global_step = cycles*epoch_size//batch_size+step\n",
    "            TrainLoss_summary_val = session.run(TrainLoss_summary, feed_dict)\n",
    "            writer.add_summary(TrainLoss_summary_val, global_step)\n",
    "            \n",
    "            total_test_loss, TestLoss_summary_val, MSE_summary_val, pMSE_summary_val = load_tree_test_data_and_test_model(test_batchesA, test_seq_lenA,test_batchesB, \n",
    "                                                                   test_seq_lenB,test_scores)\n",
    "            writer.add_summary(TestLoss_summary_val, global_step)\n",
    "            writer.add_summary(MSE_summary_val, global_step)\n",
    "            writer.add_summary(pMSE_summary_val, global_step)\n",
    "            \n",
    "            print(\"total_test_loss %.7f\" % (total_test_loss))\n",
    "            save_path = saver.save(session, \"./event_and_checkpoints/SemanticRelatednessLSTM-h.ckpt\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_batches_A = None\n",
    "sentence_batches_B = None\n",
    "sentence_len_A = None\n",
    "sentence_len_B = None\n",
    "test_scores = None \n",
    "\n",
    "def load_sentence_test_data_and_test_model(sentence_treesA, sentence_treesB, sentenceA_file_name, sentenceB_file_name, \n",
    "                                  score_file_name, output_file_name):\n",
    "    if sentence_batches_A:\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "               output_file_name)\n",
    "    else:\n",
    "        \n",
    "        if not sentenceA_file_name:\n",
    "            sentenceA_file_name = 'data/sick_trial_sentenceA.txt'\n",
    "        if not sentenceB_file_name:\n",
    "            sentenceB_file_name = 'data/sick_trial_sentenceB.txt'\n",
    "        if not score_file_name:\n",
    "            score_file_name = 'data/sick_trial_score.txt'\n",
    "        if not output_file_name:\n",
    "            output_file_name = \"./data/sick_trial_score_sentence_predict.csv\" \n",
    "\n",
    "        sentencesA = load_lines(sentenceA_file_name)\n",
    "        sentencesB = load_lines(sentenceB_file_name)\n",
    "\n",
    "        sentence_batches_A, max_sentence_lengthA, sentence_len_A = create_sentence_batches(sentencesA, sentence_treesA, batch_size)\n",
    "        sentence_batches_B, max_sentence_lengthB, sentence_len_B = create_sentence_batches(sentencesB, sentence_treesB, batch_size)\n",
    "\n",
    "        pad_sequences(sentence_batches_A, sequence_len)\n",
    "        pad_sequences(sentence_batches_B, sequence_len)\n",
    "\n",
    "        test_scores = load_scores(score_file_name, batch_size)\n",
    "\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "                   output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchesA, test_seq_lenA = None, None\n",
    "test_batchesB, test_seq_lenB = None, None\n",
    "test_scores = None\n",
    "def load_tree_test_data_and_test_model(test_batchesA_, test_seq_lenA_,test_batchesB_, test_seq_lenB_,test_scores_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
