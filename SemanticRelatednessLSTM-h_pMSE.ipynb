{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import external_lib as el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_glove_path, = el.download_and_unzip(\n",
    "#  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "#  'glove.840B.300d.txt', data_dir = \"./data_sources/glove.6B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'data/sick_filtered_glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove_path = 'data_sources/glove.6B/glove.840B.300d.txt'\n",
    "#el.filter_glove(full_glove_path, filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from data/sick_filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = el.load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\n",
    "Parse indented lines of text into a tree. Children are indented & under the parent\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert trees to a linear representation. Children are listed between the left and right \n",
    "marker in front of the parent. Each word is replaced by its id \"\"\"\n",
    "unknown_word = word_to_idx[\"UNKNOWN_WORD\"]\n",
    "left_marker = word_to_idx[\"LEFT_MARKER\"]\n",
    "right_marker = word_to_idx[\"RIGHT_MARKER\"]\n",
    "end_marker = word_to_idx[\"END_MARKER\"]\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        tree_batches.append(tree_batch)\n",
    "        batch = []\n",
    "        batches.append(batch)\n",
    "        batch_lengths = []\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for tree in tree_batch:\n",
    "            result =[]\n",
    "            batch.append(result)\n",
    "            handle_node(tree, result)\n",
    "            batch_lengths.append(len(result))\n",
    "            if len(result) > max_sequence_length:\n",
    "                max_sequence_length = len(result)\n",
    "    \n",
    "    return batches, tree_batches, max_sequence_length,batches_lengths\n",
    "                \n",
    "            \n",
    "def handle_node(node, result):\n",
    "    result.append(left_marker)\n",
    "    word = node.data[0]\n",
    "    if word in word_to_idx:\n",
    "        result.append(word_to_idx[word])\n",
    "    else:\n",
    "        result.append(unknown_word)\n",
    "        #print(\"Unknown word: \"+word)\n",
    "    if len(node.children)>0:\n",
    "        \n",
    "        for child in node.children:\n",
    "            handle_node(child, result)\n",
    "    result.append(right_marker)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pad sequences with end markers\"\"\"\n",
    "def pad_sequences(batches, max_sequence_length):\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length :\n",
    "                sentence.append(end_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert the score into a set of probabilities over the classes\"\"\"\n",
    "\n",
    "def convert_scores_to_p(scores_list):\n",
    "    scores = np.array(scores_list) \n",
    "    num_of_classes = 5 #1, 2, .. , 4, 5\n",
    "    p = np.zeros((len(scores), num_of_classes))\n",
    "    for i, score in enumerate(scores): \n",
    "        floor = math.floor(score)\n",
    "        if score == num_of_classes:\n",
    "            p[i][num_of_classes-1] = 1\n",
    "        else:\n",
    "            p[i][floor] = score - floor  #floor+1-1  zero index adjustment\n",
    "            p[i][floor-1] = floor - score + 1 - 0.015 #floor-1  zero index adjustment\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the sentences into words and convert the words to their ids\n",
    "The set of words by which to split the sentence can be found in the corresponding tree\n",
    "so fetch the set of words first \"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "def create_sentence_batches(sentences, trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        sentence_batch = sentences[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        batch = []\n",
    "        batch_lengths = []\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for j, tree in enumerate(tree_batch):\n",
    "            word_list =[]\n",
    "            get_word_list(tree, word_list)\n",
    "            #set_trace()\n",
    "            sentence_ids = []\n",
    "            batch.append(sentence_ids)\n",
    "            ordered_word_list = sentence_batch[j].replace(\",\", \" , \").replace(\".\", \" . \").replace(\"n't\", \" n't\").replace(\"'s\", \" 's \").split()\n",
    "            \n",
    "            for k in range(len(ordered_word_list)):\n",
    "                word = ordered_word_list[k]\n",
    "                if not word in word_list:\n",
    "                    print(\"missing word: \" + word)\n",
    "                    set_trace()\n",
    "                    for token in word_list:\n",
    "                        if (not token in ordered_word_list) and token in word:\n",
    "                            words = word.replace(token, \" \"+token+\" \").split()\n",
    "                            for half_word in words:\n",
    "                                if half_word in word_to_idx:\n",
    "                                    sentence_ids.append(word_to_idx[half_word])\n",
    "                                else:\n",
    "                                    sentence_ids.append(unknown_word)\n",
    "                            break\n",
    "                elif word in word_to_idx:\n",
    "                    sentence_ids.append(word_to_idx[word])\n",
    "                else:\n",
    "                    sentence_ids.append(unknown_word)\n",
    "            batch_lengths.append(len(sentence_ids))\n",
    "            if len(sentence_ids) > max_sequence_length:\n",
    "                max_sequence_length = len(sentence_ids)\n",
    "    \n",
    "    return batches, max_sequence_length, batches_lengths\n",
    "                \n",
    "            \n",
    "def get_word_list(node, word_list):\n",
    "    word = node.data[0]\n",
    "    word_list.append(word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            get_word_list(child, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(file):\n",
    "    with open(file, 'r') as f: \n",
    "        contents = f.readlines()\n",
    "        if len(contents[-1].strip())==0:\n",
    "            contents.pop(-1)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_offsets(batch_size, sequence_len, input_lengths):\n",
    "    return np.array(range(batch_size))*sequence_len + input_lengths-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "sequence_len = 100\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 5 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10\n",
    "learn_rate = 0.05\n",
    "max_grad_norm = 5\n",
    "epoch_size = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_train_sentenceA_tree.txt')\n",
    "sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_train_sentenceB_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesA, tree_batchesA, max_sequence_lengthA, seq_lenA = create_batches(sentence_treesA, batch_size)\n",
    "batchesB, tree_batchesB, max_sequence_lengthB, seq_lenB = create_batches(sentence_treesB, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = max(max_sequence_lengthA, max_sequence_lengthB)\n",
    "sequence_len_tensor = tf.constant(sequence_len, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(batchesA, sequence_len)\n",
    "pad_sequences(batchesB, sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores('data/sick_train_score.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "sentence_inputs_A = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_A\")\n",
    "sentence_inputs_A_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_A_length\")\n",
    "serial_index_offsets_A = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_A\")\n",
    "\n",
    "sentence_inputs_B = tf.placeholder(tf.int32, shape=(None, sequence_len), name=\"sentence_inputs_B\")\n",
    "sentence_inputs_B_length = tf.placeholder(tf.int32, shape=(None, ), name=\"sentence_inputs_B_length\")\n",
    "serial_index_offsets_B = tf.placeholder(tf.int32, shape=(None, ), name=\"serial_index_offsets_B\")\n",
    "\n",
    "target_score = tf.placeholder(data_type, shape=(None, output_size), name=\"target_scores\")\n",
    "target_score_scalar = tf.placeholder(data_type, shape=(None, ), name=\"target_scores\")\n",
    "\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "embedded_inputs_A = tf.nn.embedding_lookup(embedding, sentence_inputs_A)\n",
    "embedded_inputs_B = tf.nn.embedding_lookup(embedding, sentence_inputs_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def makeCells():\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        cell1 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        cell2 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    return [cell1, cell2]\n",
    "\n",
    "A =makeCells()\n",
    "B =makeCells()\n",
    "with tf.variable_scope(\"UnrolledStackedCells\", reuse=tf.AUTO_REUSE):\n",
    "    cellA = tf.contrib.rnn.MultiRNNCell(A, state_is_tuple=True)\n",
    "    outputsA, final_stateA = tf.nn.dynamic_rnn(cellA, embedded_inputs_A,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_A_length)\n",
    "    \n",
    "    cellB = tf.contrib.rnn.MultiRNNCell(B, state_is_tuple=True)\n",
    "    outputsB, final_stateB = tf.nn.dynamic_rnn(cellB, embedded_inputs_B,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_B_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'UnrolledStackedCells/rnn/transpose_1:0' shape=(?, 108, 300) dtype=float64>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_outputsA = tf.reshape(outputsA, [-1, hidden_size])\n",
    "terminal_outputsA = tf.gather(serialized_outputsA, serial_index_offsets_A)\n",
    "\n",
    "serialized_outputsB = tf.reshape(outputsB, [-1, hidden_size])\n",
    "terminal_outputsB = tf.gather(serialized_outputsB, serial_index_offsets_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.abs(tf.subtract(terminal_outputsA, terminal_outputsB))\n",
    "h_elewise_product = tf.multiply(terminal_outputsA, terminal_outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) \n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = tf.multiply(p_hat, tf.constant([1,2,3,4,5], dtype=tf.float64))\n",
    "y_hat = tf.reduce_sum(y_p, 1)\n",
    "MSE = tf.losses.mean_squared_error(target_score_scalar, y_hat)\n",
    "pMSE = tf.cast(tf.losses.mean_squared_error(target_score, p_hat), data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regularizer = tf.constant(0.0,dtype=data_type)\n",
    "for var in tf.trainable_variables(): \n",
    "    regularizer = tf.add(regularizer, tf.nn.l2_loss(var))\n",
    "loss = tf.add(pMSE, tf.multiply(tf.constant(reg_lambda, dtype=data_type), regularizer))\n",
    "\n",
    "learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),\n",
    "                                      max_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Test MSE -on Score Scalar is illegal; using Test_MSE_-on_Score_Scalar instead.\n",
      "INFO:tensorflow:Summary name Test pMSE -on probabilities is illegal; using Test_pMSE_-on_probabilities instead.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "TrainLoss_summary = tf.summary.scalar('TrainLoss', loss)\n",
    "TestLoss_summary = tf.summary.scalar('TestLoss', loss)\n",
    "MSE_summary = tf.summary.scalar('Test MSE -on Score Scalar', MSE)\n",
    "pMSE_summary = tf.summary.scalar('Test pMSE -on probabilities', pMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(batchesA, batchesB, seq_lenA, seq_lenB, scores, output_file):\n",
    "    output_scores=[]\n",
    "    total_loss = 0\n",
    "    feed_dict = {sentence_inputs_A:np.array(batchesA), sentence_inputs_A_length:np.array(seq_lenA), \n",
    "                 serial_index_offsets_A:create_index_offsets(len(batchesA), sequence_len, seq_lenA),\n",
    "                 sentence_inputs_B:np.array(batchesB), sentence_inputs_B_length:np.array(seq_lenB),\n",
    "                 serial_index_offsets_B:create_index_offsets(len(batchesB), sequence_len, seq_lenB),\n",
    "                 target_score:np.array(convert_scores_to_p(scores)), target_score_scalar:np.array(scores) }\n",
    "\n",
    "\n",
    "    fetches = {'loss': loss, 'y_hat': y_hat, 'TestLoss_summary':TestLoss_summary,\n",
    "              'MSE_summary':MSE_summary, 'pMSE_summary':pMSE_summary}\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    total_loss = vals[\"loss\"]\n",
    "    output_scores.append(vals[\"y_hat\"])\n",
    "\n",
    "\n",
    "    #print(\"Loss %.3f\" % (loss))\n",
    "\n",
    "    #import csv\n",
    "    \"\"\"with open(output_file,'w') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        for batch in output_scores:\n",
    "        for score in batch: \n",
    "            wr.writerow([repr(score)])\"\"\"\n",
    "    return total_loss, vals[\"TestLoss_summary\"], vals[\"MSE_summary\"], vals[\"pMSE_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchesA, test_seq_lenA = None, None\n",
    "test_batchesB, test_seq_lenB = None, None\n",
    "test_scores = None\n",
    "def load_tree_test_data_and_test_model(test_batchesA_, test_seq_lenA_,test_batchesB_, test_seq_lenB_,test_scores_):\n",
    "    global test_batchesA, test_seq_lenA, test_batchesB, test_seq_lenB, test_scores\n",
    "    if test_batchesA_ : \n",
    "        return test_model(test_batchesA_[0], test_batchesB_[0], test_seq_lenA_[0], test_seq_lenB_[0], test_scores_[0], \n",
    "                   \"test_results.txt\")\n",
    "    else:        \n",
    "        sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_trial_sentenceA_tree.txt')\n",
    "        sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_trial_sentenceB_tree.txt')\n",
    "\n",
    "        test_batchesA, test_tree_batchesA, max_sequence_lengthA, test_seq_lenA = create_batches(sentence_treesA,\n",
    "                                                                                                len(sentence_treesA))\n",
    "        test_batchesB, test_tree_batchesB, max_sequence_lengthB, test_seq_lenB = create_batches(sentence_treesB,\n",
    "                                                                                                len(sentence_treesB))\n",
    "\n",
    "        pad_sequences(test_batchesA, sequence_len)\n",
    "        pad_sequences(test_batchesB, sequence_len)\n",
    "\n",
    "        test_scores = load_scores('data/sick_trial_score.txt', len(sentence_treesA))\n",
    "        return test_model(test_batchesA[0], test_batchesB[0], test_seq_lenA[0], test_seq_lenB[0], test_scores[0], \n",
    "                       \"test_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_time = time.time()\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(session, \"./h_lin_tree_pMSE_train/SemanticRelatednessLSTM-h_pMSE.ckpt\")\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./h_lin_tree_pMSE_train\", session.graph)\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 perplexity: 1.198 speed: 18 wps\n",
      "100*Loss 18.081\n",
      "total_test_loss 0.1838260\n",
      "0.002 perplexity: 1.195 speed: 71 wps\n",
      "100*Loss 18.126\n",
      "total_test_loss 0.1828777\n",
      "0.012 perplexity: 1.196 speed: 141 wps\n",
      "100*Loss 18.152\n",
      "total_test_loss 0.1797041\n",
      "0.022 perplexity: 1.197 speed: 168 wps\n",
      "100*Loss 18.093\n",
      "total_test_loss 0.1772605\n",
      "0.032 perplexity: 1.195 speed: 163 wps\n",
      "100*Loss 16.768\n",
      "total_test_loss 0.1745587\n",
      "0.002 perplexity: 1.193 speed: 160 wps\n",
      "100*Loss 17.050\n",
      "total_test_loss 0.1723964\n",
      "0.012 perplexity: 1.192 speed: 164 wps\n",
      "100*Loss 16.807\n",
      "total_test_loss 0.1704575\n",
      "0.022 perplexity: 1.191 speed: 171 wps\n",
      "100*Loss 17.574\n",
      "total_test_loss 0.1691119\n",
      "0.032 perplexity: 1.190 speed: 168 wps\n",
      "100*Loss 15.076\n",
      "total_test_loss 0.1676572\n",
      "0.002 perplexity: 1.188 speed: 165 wps\n",
      "100*Loss 16.437\n",
      "total_test_loss 0.1666839\n",
      "0.012 perplexity: 1.187 speed: 167 wps\n",
      "100*Loss 16.003\n",
      "total_test_loss 0.1659916\n",
      "0.022 perplexity: 1.187 speed: 171 wps\n",
      "100*Loss 17.383\n",
      "total_test_loss 0.1656706\n",
      "0.032 perplexity: 1.186 speed: 170 wps\n",
      "100*Loss 14.114\n",
      "total_test_loss 0.1652356\n",
      "0.002 perplexity: 1.185 speed: 168 wps\n",
      "100*Loss 16.221\n",
      "total_test_loss 0.1650326\n",
      "0.012 perplexity: 1.184 speed: 169 wps\n",
      "100*Loss 15.689\n",
      "total_test_loss 0.1648685\n",
      "0.022 perplexity: 1.184 speed: 172 wps\n",
      "100*Loss 17.384\n",
      "total_test_loss 0.1647987\n",
      "0.032 perplexity: 1.184 speed: 170 wps\n",
      "100*Loss 13.753\n",
      "total_test_loss 0.1646355\n",
      "0.002 perplexity: 1.183 speed: 169 wps\n",
      "100*Loss 16.139\n",
      "total_test_loss 0.1645920\n",
      "0.012 perplexity: 1.183 speed: 170 wps\n",
      "100*Loss 15.564\n",
      "total_test_loss 0.1645111\n",
      "0.022 perplexity: 1.183 speed: 172 wps\n",
      "100*Loss 17.406\n",
      "total_test_loss 0.1644444\n",
      "0.032 perplexity: 1.182 speed: 170 wps\n",
      "100*Loss 13.620\n",
      "total_test_loss 0.1643383\n",
      "0.002 perplexity: 1.182 speed: 169 wps\n",
      "100*Loss 16.087\n",
      "total_test_loss 0.1643264\n",
      "0.012 perplexity: 1.182 speed: 170 wps\n",
      "100*Loss 15.498\n",
      "total_test_loss 0.1642616\n",
      "0.022 perplexity: 1.182 speed: 171 wps\n",
      "100*Loss 17.420\n",
      "total_test_loss 0.1641842\n",
      "0.032 perplexity: 1.181 speed: 170 wps\n",
      "100*Loss 13.558\n",
      "total_test_loss 0.1640993\n",
      "0.002 perplexity: 1.181 speed: 169 wps\n",
      "100*Loss 16.049\n",
      "total_test_loss 0.1640989\n",
      "0.012 perplexity: 1.181 speed: 170 wps\n",
      "100*Loss 15.455\n",
      "total_test_loss 0.1640409\n",
      "0.022 perplexity: 1.181 speed: 171 wps\n",
      "100*Loss 17.427\n",
      "total_test_loss 0.1639574\n",
      "0.032 perplexity: 1.181 speed: 171 wps\n",
      "100*Loss 13.519\n",
      "total_test_loss 0.1638836\n",
      "0.002 perplexity: 1.180 speed: 170 wps\n",
      "100*Loss 16.018\n",
      "total_test_loss 0.1638889\n",
      "0.012 perplexity: 1.180 speed: 170 wps\n",
      "100*Loss 15.421\n",
      "total_test_loss 0.1638350\n",
      "0.022 perplexity: 1.180 speed: 172 wps\n",
      "100*Loss 17.427\n",
      "total_test_loss 0.1637479\n",
      "0.032 perplexity: 1.180 speed: 171 wps\n",
      "100*Loss 13.490\n",
      "total_test_loss 0.1636810\n",
      "0.002 perplexity: 1.180 speed: 170 wps\n",
      "100*Loss 15.991\n",
      "total_test_loss 0.1636892\n",
      "0.012 perplexity: 1.180 speed: 171 wps\n",
      "100*Loss 15.394\n",
      "total_test_loss 0.1636384\n",
      "0.022 perplexity: 1.180 speed: 171 wps\n",
      "100*Loss 17.423\n",
      "total_test_loss 0.1635473\n",
      "0.032 perplexity: 1.180 speed: 171 wps\n",
      "100*Loss 13.465\n",
      "total_test_loss 0.1634872\n",
      "0.002 perplexity: 1.179 speed: 170 wps\n",
      "100*Loss 15.967\n",
      "total_test_loss 0.1634969\n",
      "0.012 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 15.371\n",
      "total_test_loss 0.1634502\n",
      "0.022 perplexity: 1.179 speed: 172 wps\n",
      "100*Loss 17.399\n",
      "total_test_loss 0.1633594\n",
      "0.032 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 13.473\n",
      "total_test_loss 0.1633154\n",
      "0.002 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 15.945\n",
      "total_test_loss 0.1633282\n",
      "0.012 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 15.346\n",
      "total_test_loss 0.1632805\n",
      "0.022 perplexity: 1.179 speed: 172 wps\n",
      "100*Loss 17.392\n",
      "total_test_loss 0.1631741\n",
      "0.032 perplexity: 1.179 speed: 172 wps\n",
      "100*Loss 13.433\n",
      "total_test_loss 0.1631478\n",
      "0.002 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 15.923\n",
      "total_test_loss 0.1631519\n",
      "0.012 perplexity: 1.179 speed: 171 wps\n",
      "100*Loss 15.326\n",
      "total_test_loss 0.1631058\n",
      "0.022 perplexity: 1.179 speed: 172 wps\n",
      "100*Loss 17.381\n",
      "total_test_loss 0.1630057\n",
      "0.032 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 13.370\n",
      "total_test_loss 0.1629744\n",
      "0.002 perplexity: 1.178 speed: 171 wps\n",
      "100*Loss 15.901\n",
      "total_test_loss 0.1629782\n",
      "0.012 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.307\n",
      "total_test_loss 0.1629340\n",
      "0.022 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 17.372\n",
      "total_test_loss 0.1628255\n",
      "0.032 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 13.388\n",
      "total_test_loss 0.1627957\n",
      "0.002 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.880\n",
      "total_test_loss 0.1628067\n",
      "0.012 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.289\n",
      "total_test_loss 0.1627660\n",
      "0.022 perplexity: 1.178 speed: 173 wps\n",
      "100*Loss 17.362\n",
      "total_test_loss 0.1626532\n",
      "0.032 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 13.376\n",
      "total_test_loss 0.1626140\n",
      "0.002 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.859\n",
      "total_test_loss 0.1626382\n",
      "0.012 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.273\n",
      "total_test_loss 0.1626017\n",
      "0.022 perplexity: 1.178 speed: 173 wps\n",
      "100*Loss 17.352\n",
      "total_test_loss 0.1624853\n",
      "0.032 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 13.356\n",
      "total_test_loss 0.1624581\n",
      "0.002 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.840\n",
      "total_test_loss 0.1624689\n",
      "0.012 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 15.258\n",
      "total_test_loss 0.1624382\n",
      "0.022 perplexity: 1.178 speed: 173 wps\n",
      "100*Loss 17.339\n",
      "total_test_loss 0.1623227\n",
      "0.032 perplexity: 1.178 speed: 172 wps\n",
      "100*Loss 13.332\n",
      "total_test_loss 0.1622918\n",
      "0.002 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.821\n",
      "total_test_loss 0.1623027\n",
      "0.012 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.245\n",
      "total_test_loss 0.1622685\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.328\n",
      "total_test_loss 0.1621623\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.310\n",
      "total_test_loss 0.1621339\n",
      "0.002 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.802\n",
      "total_test_loss 0.1621425\n",
      "0.012 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.228\n",
      "total_test_loss 0.1621132\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.315\n",
      "total_test_loss 0.1620031\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.288\n",
      "total_test_loss 0.1619784\n",
      "0.002 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.783\n",
      "total_test_loss 0.1619828\n",
      "0.012 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.214\n",
      "total_test_loss 0.1619562\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.300\n",
      "total_test_loss 0.1618441\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.271\n",
      "total_test_loss 0.1618219\n",
      "0.002 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.764\n",
      "total_test_loss 0.1618249\n",
      "0.012 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 15.199\n",
      "total_test_loss 0.1618008\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.285\n",
      "total_test_loss 0.1616864\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.254\n",
      "total_test_loss 0.1616662\n",
      "0.002 perplexity: 1.177 speed: 172 wps\n",
      "100*Loss 15.746\n",
      "total_test_loss 0.1616676\n",
      "0.012 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 15.185\n",
      "total_test_loss 0.1616463\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.271\n",
      "total_test_loss 0.1615298\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.237\n",
      "total_test_loss 0.1615113\n",
      "0.002 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 15.728\n",
      "total_test_loss 0.1615104\n",
      "0.012 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 15.170\n",
      "total_test_loss 0.1614928\n",
      "0.022 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 17.255\n",
      "total_test_loss 0.1613742\n",
      "0.032 perplexity: 1.177 speed: 173 wps\n",
      "100*Loss 13.219\n",
      "total_test_loss 0.1613577\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.710\n",
      "total_test_loss 0.1613546\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.156\n",
      "total_test_loss 0.1613406\n",
      "0.022 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 17.240\n",
      "total_test_loss 0.1612196\n",
      "0.032 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 13.201\n",
      "total_test_loss 0.1612050\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.692\n",
      "total_test_loss 0.1611994\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.141\n",
      "total_test_loss 0.1611893\n",
      "0.022 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 17.224\n",
      "total_test_loss 0.1610657\n",
      "0.032 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 13.184\n",
      "total_test_loss 0.1610530\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.675\n",
      "total_test_loss 0.1610449\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.127\n",
      "total_test_loss 0.1610389\n",
      "0.022 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 17.209\n",
      "total_test_loss 0.1609125\n",
      "0.032 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 13.166\n",
      "total_test_loss 0.1609019\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_loss 0.1608910\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.112\n",
      "total_test_loss 0.1608896\n",
      "0.022 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 17.193\n",
      "total_test_loss 0.1607598\n",
      "0.032 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 13.148\n",
      "total_test_loss 0.1607517\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.641\n",
      "total_test_loss 0.1607372\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.098\n",
      "total_test_loss 0.1607412\n",
      "0.022 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 17.177\n",
      "total_test_loss 0.1606079\n",
      "0.032 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 13.130\n",
      "total_test_loss 0.1606019\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.624\n",
      "total_test_loss 0.1605836\n",
      "0.012 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.084\n",
      "total_test_loss 0.1605929\n",
      "0.022 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 17.161\n",
      "total_test_loss 0.1604565\n",
      "0.032 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 13.111\n",
      "total_test_loss 0.1604524\n",
      "0.002 perplexity: 1.176 speed: 173 wps\n",
      "100*Loss 15.608\n",
      "total_test_loss 0.1604304\n",
      "0.012 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 15.070\n",
      "total_test_loss 0.1604451\n",
      "0.022 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 17.145\n",
      "total_test_loss 0.1603057\n",
      "0.032 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 13.092\n",
      "total_test_loss 0.1603038\n",
      "0.002 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 15.593\n",
      "total_test_loss 0.1602770\n",
      "0.012 perplexity: 1.175 speed: 174 wps\n",
      "100*Loss 15.055\n",
      "total_test_loss 0.1602983\n",
      "0.022 perplexity: 1.176 speed: 174 wps\n",
      "100*Loss 17.129\n",
      "total_test_loss 0.1601551\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for cycles in range(10000):\n",
    "    for step in range(epoch_size//batch_size):\n",
    "        feed_dict = {sentence_inputs_A:np.array(batchesA[step]), sentence_inputs_A_length:np.array(seq_lenA[step]),\n",
    "                     serial_index_offsets_A:create_index_offsets(len(batchesA[step]), sequence_len, seq_lenA[step]),\n",
    "                     sentence_inputs_B:np.array(batchesB[step]), sentence_inputs_B_length:np.array(seq_lenB[step]),\n",
    "                     serial_index_offsets_B:create_index_offsets(len(batchesB[step]), sequence_len, seq_lenB[step]),\n",
    "                     target_score:np.array(convert_scores_to_p(scores[step])), target_score_scalar:np.array(scores[step])}\n",
    "\n",
    "        fetches = {'loss': loss, 'train_op':train_op}\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"loss\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters +=  1\n",
    "\n",
    "        if (cycles == 0 and step == 0 ) or (step % (epoch_size // 100) == 10):\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                    (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                    iters * batch_size * max(1, 1) /\n",
    "                    (time.time() - start_time)))\n",
    "            print(\"100*Loss %.3f\" % (100*cost))\n",
    "            \n",
    "            global_step = cycles*epoch_size//batch_size+step\n",
    "            TrainLoss_summary_val = session.run(TrainLoss_summary, feed_dict)\n",
    "            writer.add_summary(TrainLoss_summary_val, global_step)\n",
    "            \n",
    "            total_test_loss, TestLoss_summary_val, MSE_summary_val, pMSE_summary_val = load_tree_test_data_and_test_model(test_batchesA, test_seq_lenA,test_batchesB, \n",
    "                                                                   test_seq_lenB,test_scores)\n",
    "            writer.add_summary(TestLoss_summary_val, global_step)\n",
    "            writer.add_summary(MSE_summary_val, global_step)\n",
    "            writer.add_summary(pMSE_summary_val, global_step)\n",
    "            \n",
    "            print(\"total_test_loss %.7f\" % (total_test_loss))\n",
    "            save_path = saver.save(session, \"./h_lin_tree_pMSE_train/SemanticRelatednessLSTM-h.ckpt\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_batches_A = None\n",
    "sentence_batches_B = None\n",
    "sentence_len_A = None\n",
    "sentence_len_B = None\n",
    "test_scores = None \n",
    "\n",
    "def load_sentence_test_data_and_test_model(sentence_treesA, sentence_treesB, sentenceA_file_name, sentenceB_file_name, \n",
    "                                  score_file_name, output_file_name):\n",
    "    if sentence_batches_A:\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "               output_file_name)\n",
    "    else:\n",
    "        \n",
    "        if not sentenceA_file_name:\n",
    "            sentenceA_file_name = 'data/sick_trial_sentenceA.txt'\n",
    "        if not sentenceB_file_name:\n",
    "            sentenceB_file_name = 'data/sick_trial_sentenceB.txt'\n",
    "        if not score_file_name:\n",
    "            score_file_name = 'data/sick_trial_score.txt'\n",
    "        if not output_file_name:\n",
    "            output_file_name = \"./data/sick_trial_score_sentence_predict.csv\" \n",
    "\n",
    "        sentencesA = load_lines(sentenceA_file_name)\n",
    "        sentencesB = load_lines(sentenceB_file_name)\n",
    "\n",
    "        sentence_batches_A, max_sentence_lengthA, sentence_len_A = create_sentence_batches(sentencesA, sentence_treesA, batch_size)\n",
    "        sentence_batches_B, max_sentence_lengthB, sentence_len_B = create_sentence_batches(sentencesB, sentence_treesB, batch_size)\n",
    "\n",
    "        pad_sequences(sentence_batches_A, sequence_len)\n",
    "        pad_sequences(sentence_batches_B, sequence_len)\n",
    "\n",
    "        test_scores = load_scores(score_file_name, batch_size)\n",
    "\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "                   output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
