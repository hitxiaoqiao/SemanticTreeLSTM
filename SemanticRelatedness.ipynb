{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import ChildSumTree\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files to /var/folders/_4/p7lkljg13nvbgmfj_qvgf3vm0000gn/T/tmpwiweseip\n"
     ]
    }
   ],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "print('saving files to %s' % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url_base, zip_name, *file_names):\n",
    "  zip_path = os.path.join(data_dir, zip_name)\n",
    "  url = url_base + zip_name\n",
    "  print('downloading %s to %s' % (url, zip_path))\n",
    "  urllib.request.urlretrieve(url, zip_path)\n",
    "  out_paths = []\n",
    "  with zipfile.ZipFile(zip_path, 'r') as f:\n",
    "    for file_name in file_names:\n",
    "      print('extracting %s' % file_name)\n",
    "      out_paths.append(f.extract(file_name, path=data_dir))\n",
    "  return out_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading http://nlp.stanford.edu/data/glove.840B.300d.zip to /var/folders/_4/p7lkljg13nvbgmfj_qvgf3vm0000gn/T/tmpwiweseip/glove.840B.300d.zip\n",
      "extracting glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "full_glove_path, = download_and_unzip(\n",
    "  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "  'glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'sick_filtered_glove2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_glove():\n",
    "    vocab = set()\n",
    "    # Download the full set of unlabeled sentences separated by '|'.\n",
    "    sentence_path = ['sick_train_sentenceA_tree.txt', 'sick_train_sentenceB_tree.txt',\n",
    "                      'sick_trial_sentenceA_tree.txt', 'sick_trial_sentenceB_tree.txt',]\n",
    "    for path in sentence_path:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                # Drop the trailing newline and strip backslashes. Split into words.\n",
    "                vocab.update(line.strip().split())\n",
    "    nread = 0\n",
    "    nwrote = 0\n",
    "    with codecs.open(full_glove_path, encoding='utf-8') as f:\n",
    "        with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                nread += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    nwrote += 1\n",
    "    print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2196018 lines, wrote 2335\n"
     ]
    }
   ],
   "source": [
    "vocab = filter_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_path):\n",
    "    \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "    # '-RRB-' respectively in the parse-trees.\n",
    "    if u'(' in word_idx:\n",
    "        word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    if u')' in word_idx:\n",
    "        word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "    # Random embedding vector for unknown words.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from sick_filtered_glove2.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = load_embeddings(filtered_glove_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2336, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "batch_size = None\n",
    "data_type = tf.float64\n",
    "output_size = 21 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "batch_size = tf.placeholder(tf.int32, shape=(1,), name=\"batch_size\")\n",
    "inputs_idx = tf.placeholder(tf.int32, name=\"inputs_idx\")\n",
    "embedding = tf.constant(embedding_matrix)\n",
    "input_batch = tf.nn.embedding_lookup(embedding, inputs_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer basic_lstm_cell_1 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4b38e42b6ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         [make_cell() for _ in range(num_layers)], state_is_tuple=True)\n\u001b[1;32m      9\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1323\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1324\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 339\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m-> 1381\u001b[0;31m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m                            \u001b[0;34m'its rank is undefined, but the layer requires a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                            'defined rank.')\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer basic_lstm_cell_1 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank."
     ]
    }
   ],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def make_cell():\n",
    "\tcell = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, reuse=False)\n",
    "\treturn cell\n",
    "               \n",
    "cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [make_cell() for _ in range(num_layers)], state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, data_type)\n",
    "outputs, final_state = cell(input_batch, state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_abs_difference = tf.placeholder(data_type, name=\"h_abs_difference\")\n",
    "h_elewise_product = tf.placeholder(data_type, name=\"h_elewise_product\")\n",
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.nn.xw_plus_b(h_elewise_product, W_h_elewise_product, h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) #TODO what's the correct shape??\n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))\n",
    "\n",
    "r = np.linspace(1.0,5.0,41) #1.0, 1.1, 1.2 ... 4.9, 5.0\n",
    "y_hat = tf.matmul(r, p_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"split the tree nodes into batches and stages/generations of children\"\"\"\n",
    "#TODO Shuffle the tree lists and the scores\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    tree_batches = []\n",
    "    stage_batches = []\n",
    "    sentence_trees = np.array(trees)\n",
    "    for i in range(len(sentence_trees)//tree_batch_size):\n",
    "        tree_batch = sentence_trees[i*tree_batch_size:(i+1)*tree_batch_size] \n",
    "        tree_batches.append(tree_batch)\n",
    "        stage_batch = []\n",
    "        stage_batches.append(stage_batch)\n",
    "        for tree in tree_batch:\n",
    "            nodes = [tree]\n",
    "            generations = []\n",
    "            while len(nodes) > 0:\n",
    "                generations.append([]+nodes)\n",
    "                next_nodes=[]\n",
    "                for node in nodes:\n",
    "                    next_nodes.extend(node.children)\n",
    "                nodes = next_nodes\n",
    "            current_stage = 0\n",
    "            for generation in reversed(generations):\n",
    "                if len(stage_batch)<current_stage+1:\n",
    "                    stage_batch.append(generation)\n",
    "                else:\n",
    "                    stage_batch[current_stage].extend(generation)\n",
    "                current_stage += 1\n",
    "    return stage_batches, tree_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO each node must know it's location \n",
    "\n",
    "def pad_batches(stage_batches, tree_batches):\n",
    "    generation_size = len(stage_batches[0][0])\n",
    "    remove_idx = [1, 0, 0]\n",
    "    for sb_idx, stage_batch in enumerate(stage_batches):\n",
    "        for g_idx, generation in enumerate(stage_batch):\n",
    "            while (not len(generation)==0) and len(generation) < generation_size:\n",
    "                if remove_idx[0] < len(stage_batches) and remove_idx[0] > sb_idx: \n",
    "                    \"\"\"Pad with other nodes\"\"\"\n",
    "                    if len(stage_batches[remove_idx[0]][remove_idx[1]]) > 0:\n",
    "                        generation.append(stage_batches[remove_idx[0]][remove_idx[1]].pop(0))\n",
    "                    elif remove_idx[1] + 1 < len(stage_batches[remove_idx[0]]):\n",
    "                        remove_idx[1] = remove_idx[1] + 1\n",
    "                    else:\n",
    "                        remove_idx[0] = remove_idx[0] + 1\n",
    "                        remove_idx[1] = 0\n",
    "                else: \n",
    "                    \"\"\"pad_with_zeros\"\"\"\n",
    "                    generation.append(Node([], parent=None))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_scores('sick_train_score.txt', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unstitching\n",
      "uninterestedly\n",
      "midspeech\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convert words to ids\n",
    " Takes batches of tree nodes arranged in stages/generations\n",
    " returns the same data structure as the input except instead of nodes there are ids\"\"\"\n",
    "def covert_words_ids(stage_batches, word_to_idx, embedding_matrix):    \n",
    "    stage_batches_ids = []\n",
    "    for stage_batch in stage_batches:\n",
    "        stage_batch_ids = []\n",
    "        stage_batches_ids.append(stage_batch_ids)\n",
    "        for idx, generation in enumerate(stage_batch):\n",
    "            generation_ids = []\n",
    "            stage_batch_ids.append(generation_ids)\n",
    "            for node in generation:\n",
    "                word = node.data[0]\n",
    "                if word in word_to_idx:\n",
    "                    node_word_id = word_to_idx[word]\n",
    "                    generation_ids.append(node_word_id)\n",
    "                else:\n",
    "                    generation_ids.append(len(embedding_matrix)-1)\n",
    "                    print(\"unknown: \"+word)\n",
    "    return stage_batches_ids\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"takes the target score array and converts them to probabilities \"\"\"\n",
    "def convert_scores_to_p(scores, num_of_classes=5):\n",
    "    p = np.zeros((len(scores), num_of_classes*(num_of_classes-1)+1))\n",
    "    for i, score in enumerate(scores):        \n",
    "        sim = score * (num_of_classes - 1) + 1\n",
    "        floor = math.floor(sim) \n",
    "        ceil = math.ceil(sim) \n",
    "        if math.ceil(sim) == math.floor(sim):\n",
    "            p[i][ceil] = 1\n",
    "        else:\n",
    "            p[i][floor] = ceil - sim\n",
    "            p[i][ceil] = sim - floor\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2336, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Feed the data in, run the session and prepare the final state for the next generation \n",
    "   Repeat until every root in the batch has an output\n",
    "   Calculate the loss and optimize \"\"\"\n",
    "epochs = 1000\n",
    "writer = tf.summary.FileWriter(\"./\", session.graph)\n",
    "state = session.run(initial_state, {batch_size:})\n",
    "for i in range(epochs): \n",
    "    for i, batch in enumerate(stage_batches_ids): \n",
    "        for j, generation in enumerate(batch):\n",
    "            \"\"\"Feed generation in and get output and states\"\"\"\n",
    "            feed_dict = {inputs:, targets:}\n",
    "            if j == 0: \n",
    "                \"\"\"Use zerostate\"\"\"\n",
    "                state = session.run(initial_state, {batch_size:len(generation)})\n",
    "                for i, (c, h) in enumerate(initial_state):\n",
    "                    feed_dict[c] = state[i][c]\n",
    "                    feed_dict[h] = state[i][h]\n",
    "            else: \n",
    "                \"\"\"generate the state from the children of the nodes in the next bactch\"\"\"\n",
    "                for i, (c, h) in enumerate(initial_state):\n",
    "                    \n",
    "            fetches = {outputs, final_state}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
