{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import ChildSumTree as cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files to /var/folders/_4/p7lkljg13nvbgmfj_qvgf3vm0000gn/T/tmp68o_vrpu\n"
     ]
    }
   ],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "print('saving files to %s' % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url_base, zip_name, *file_names):\n",
    "  zip_path = os.path.join(data_dir, zip_name)\n",
    "  url = url_base + zip_name\n",
    "  print('downloading %s to %s' % (url, zip_path))\n",
    "  urllib.request.urlretrieve(url, zip_path)\n",
    "  out_paths = []\n",
    "  with zipfile.ZipFile(zip_path, 'r') as f:\n",
    "    for file_name in file_names:\n",
    "      print('extracting %s' % file_name)\n",
    "      out_paths.append(f.extract(file_name, path=data_dir))\n",
    "  return out_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading http://nlp.stanford.edu/data/glove.840B.300d.zip to /var/folders/_4/p7lkljg13nvbgmfj_qvgf3vm0000gn/T/tmpwiweseip/glove.840B.300d.zip\n",
      "extracting glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "full_glove_path, = download_and_unzip(\n",
    "  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "  'glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'sick_filtered_glove2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_glove():\n",
    "    vocab = set()\n",
    "    # Download the full set of unlabeled sentences separated by '|'.\n",
    "    sentence_path = ['sick_train_sentenceA_tree.txt', 'sick_train_sentenceB_tree.txt',\n",
    "                      'sick_trial_sentenceA_tree.txt', 'sick_trial_sentenceB_tree.txt',]\n",
    "    for path in sentence_path:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                # Drop the trailing newline and strip backslashes. Split into words.\n",
    "                vocab.update(line.strip().split())\n",
    "    nread = 0\n",
    "    nwrote = 0\n",
    "    with codecs.open(full_glove_path, encoding='utf-8') as f:\n",
    "        with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                nread += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    nwrote += 1\n",
    "    print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_glove_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f0d58d6343e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0a67d6def0d5>\u001b[0m in \u001b[0;36mfilter_glove\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnwrote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_glove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_glove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_glove_path' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = filter_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_path):\n",
    "    \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "    # '-RRB-' respectively in the parse-trees.\n",
    "    if u'(' in word_idx:\n",
    "        word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    if u')' in word_idx:\n",
    "        word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "    # Random embedding vector for unknown words.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from sick_filtered_glove2.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = load_embeddings(filtered_glove_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2336, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(25, 300) dtype=float64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'Layer2/Add_28:0' shape=(25, 300) dtype=float64>, h=<tf.Tensor 'Layer2/Mul_11:0' shape=(25, 300) dtype=float64>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"split the tree nodes into batches and stages/generations of children\"\"\"\n",
    "#TODO Shuffle the tree lists and the scores\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    tree_batches = []\n",
    "    stage_batches = []\n",
    "    max_gen_length = 0\n",
    "    sentence_trees = np.array(trees)\n",
    "    for i in range(len(sentence_trees)//tree_batch_size):\n",
    "        #create a batch of tree_batch_size trees\n",
    "        tree_batch = sentence_trees[i*tree_batch_size:(i+1)*tree_batch_size] \n",
    "        tree_batches.append(tree_batch)\n",
    "        stage_batch = []\n",
    "        stage_batches.append(stage_batch)\n",
    "        for tree in tree_batch:\n",
    "            nodes = [tree]\n",
    "            generations = []\n",
    "            while len(nodes) > 0:\n",
    "                generations.append([]+nodes)\n",
    "                next_nodes=[]\n",
    "                for node in nodes:\n",
    "                    next_nodes.extend(node.children)\n",
    "                nodes = next_nodes\n",
    "            current_stage = 0\n",
    "            for generation in reversed(generations):\n",
    "                if len(stage_batch)<current_stage+1:\n",
    "                    stage_batch.append(generation)\n",
    "                else:\n",
    "                    stage_batch[current_stage].extend(generation)\n",
    "                if len(stage_batch[current_stage])>max_gen_length:\n",
    "                    max_gen_length = len(stage_batch[current_stage])\n",
    "                current_stage += 1\n",
    "    return stage_batches, tree_batches, max_gen_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO each node must know it's location \n",
    "\n",
    "def pad_batches(stage_batches, generation_size):\n",
    "    for sb_idx, stage_batch in enumerate(stage_batches):\n",
    "        for g_idx, generation in enumerate(stage_batch):\n",
    "            while (not len(generation)==0) and len(generation) < generation_size:\n",
    "                generation.append(Node(['.'], parent=None))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_scores('sick_train_score.txt', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert words to ids\n",
    " Takes batches of tree nodes arranged in stages/generations\n",
    " returns the same data structure as the input except instead of nodes there are ids\"\"\"\n",
    "def covert_words_ids(stage_batches, word_to_idx, unknown_word_id):    \n",
    "    stage_batches_ids = []\n",
    "    for batch_idx, stage_batch in enumerate(stage_batches):\n",
    "        stage_batch_ids = []\n",
    "        stage_batches_ids.append(stage_batch_ids)\n",
    "        for gen_idx, generation in enumerate(stage_batch):\n",
    "            generation_ids = []\n",
    "            stage_batch_ids.append(generation_ids)\n",
    "            for node_idx, node in enumerate(generation):\n",
    "                word = node.data[0]\n",
    "                node.data.append((batch_idx, gen_idx, node_idx))\n",
    "                if word in word_to_idx:\n",
    "                    node_word_id = word_to_idx[word]\n",
    "                    generation_ids.append(node_word_id)\n",
    "                else:\n",
    "                    generation_ids.append(unknown_word_id)\n",
    "                    print(\"unknown: \"+word)\n",
    "    return stage_batches_ids\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"takes the target score array and converts them to probabilities \"\"\"\n",
    "def convert_scores_to_p(scores, num_of_classes=5):\n",
    "    p = np.zeros((len(scores), num_of_classes*(num_of_classes-1)+1))\n",
    "    for i, score in enumerate(scores):        \n",
    "        sim = score * (num_of_classes - 1) + 1\n",
    "        floor = math.floor(sim) \n",
    "        ceil = math.ceil(sim) \n",
    "        if math.ceil(sim) == math.floor(sim):\n",
    "            p[i][ceil] = 1\n",
    "        else:\n",
    "            p[i][floor] = ceil - sim\n",
    "            p[i][ceil] = sim - floor\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 21 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_trees, max_children = parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_batches, tree_batches, max_gen_length = create_batches(sentence_trees, tree_batch_size = 25)\n",
    "batch_size=max_gen_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_batches(stage_batches, max_gen_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown: unstitching\n",
      "unknown: uninterestedly\n",
      "unknown: midspeech\n"
     ]
    }
   ],
   "source": [
    "unknown_word_id = len(embedding_matrix)-1\n",
    "stage_batches_ids = covert_words_ids(stage_batches, word_to_idx, unknown_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "inputs_idx = tf.placeholder(tf.int32, shape=(batch_size, ), name=\"inputs_idx\")\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "input_batch = tf.nn.embedding_lookup(embedding, inputs_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "proto_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
    "          reuse=False)\n",
    "\n",
    "cell_layer_1 = cst.ChildSumTreeLSTMCell(hidden_size)\n",
    "initial_state1 = [proto_cell.zero_state(batch_size, data_type) for i in range(max_children)]\n",
    "outputs1, final_state1 = cell_layer_1(input_batch, state=initial_state1, scope='Layer1')\n",
    "cell_layer_2 = cst.ChildSumTreeLSTMCell(hidden_size)\n",
    "initial_state2 = [proto_cell.zero_state(batch_size, data_type) for i in range(max_children)]\n",
    "outputs2, final_state2 = cell_layer_2(outputs1, state=initial_state2, scope='Layer2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.placeholder(data_type, name=\"h_abs_difference\")\n",
    "h_elewise_product = tf.placeholder(data_type, name=\"h_elewise_product\")\n",
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) #TODO what's the correct shape??\n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))\n",
    "\n",
    "#r = np.linspace(1.0,5.0,21) #1.0, 1.1, 1.2 ... 4.9, 5.0\n",
    "#y_hat = tf.matmul(r, p_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "gen_length = 109\n",
    "for sb_idx, stage_batch in enumerate(stage_batches):\n",
    "    for g_idx, generation in enumerate(stage_batch):\n",
    "        if len(generation)<gen_length:\n",
    "            gen_length = len(generation)\n",
    "print(gen_length)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43875\n"
     ]
    }
   ],
   "source": [
    "nodes = 0\n",
    "for sb_idx, stage_batch in enumerate(stage_batches):\n",
    "    for g_idx, generation in enumerate(stage_batch):\n",
    "        nodes += len(generation)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "node_count = 0\n",
    "for sb_idx, batch in enumerate(tree_batches):\n",
    "    for g_idx, node in enumerate(batch):\n",
    "        node_q = [node]\n",
    "        while len(node_q)>0:\n",
    "            current_node = node_q.pop(0)\n",
    "            node_q.extend(current_node.children)\n",
    "            node_count +=1\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feed the data in, run the session and prepare the final state for the next generation \n",
    "   Repeat until every root in the batch has an output\n",
    "   Calculate the loss and optimize \"\"\"\n",
    "epochs = 1000\n",
    "writer = tf.summary.FileWriter(\"./\", session.graph)\n",
    "state1 = session.run(initial_state1)\n",
    "state2 = session.run(initial_state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    " \n",
    "for epoch_idx in range(epochs): \n",
    "    for batch_idx, batch in enumerate(stage_batches_ids): \n",
    "        for gen_idx, generation in enumerate(batch):\n",
    "            \"\"\"Feed generation in and get output and states\"\"\"\n",
    "            feed_dict = {inputs_idx:generation}\n",
    "            if gen_idx == 0: \n",
    "                \"\"\"Use zerostate\"\"\"\n",
    "                \n",
    "                for child_i, (c, h) in enumerate(initial_state1):\n",
    "                    feed_dict[c] = state1[child_i].c\n",
    "                    feed_dict[h] = state1[child_i].h\n",
    "                for child_i, (c, h) in enumerate(initial_state2):\n",
    "                    feed_dict[c] = state2[child_i].c\n",
    "                    feed_dict[h] = state2[child_i].h\n",
    "            else: \n",
    "                \"\"\"generate the state from the children of the nodes in the next batch\"\"\"\n",
    "                zero_state1 = session.run(initial_state1) \n",
    "                zero_state2 = session.run(initial_state2) #I\n",
    "                node_batch = stage_batches[batch_idx][gen_idx]\n",
    "                for node_idx, node in enumerate(batch):\n",
    "                    children = node_batch[node_idx].children\n",
    "                    for i, child in  enumerate(children):\n",
    "                        zero_state1[i].c[node_idx]= state1.c[child.data[3][2]]\n",
    "                        zero_state1[i].h[node_idx]= state1.h[child.data[3][2]]\n",
    "                        zero_state2[i].c[node_idx]= state2.c[child.data[3][2]]\n",
    "                        zero_state2[i].h[node_idx]= state2.h[child.data[3][2]]\n",
    "                for child_i, (c, h) in enumerate(initial_state1):\n",
    "                    feed_dict[c] = state1[child_i].c\n",
    "                    feed_dict[h] = state1[child_i].h\n",
    "                for child_i, (c, h) in enumerate(initial_state2):\n",
    "                    feed_dict[c] = state2[child_i].c\n",
    "                    feed_dict[h] = state2[child_i].h\n",
    "            \n",
    "            \n",
    "            fetches = {'outputs1':outputs1, 'outputs2':outputs2, 'final_state1':final_state1, 'final_state2':final_state2}\n",
    "            results = session.run(fetches, feed_dict)\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
