{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files to /var/folders/_4/p7lkljg13nvbgmfj_qvgf3vm0000gn/T/tmp9r03rwj7\n"
     ]
    }
   ],
   "source": [
    "data_dir = tempfile.mkdtemp()\n",
    "print('saving files to %s' % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url_base, zip_name, *file_names):\n",
    "    zip_path = os.path.join(data_dir, zip_name)\n",
    "    url = url_base + zip_name\n",
    "    print('downloading %s to %s' % (url, zip_path))\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    out_paths = []\n",
    "    with zipfile.ZipFile(zip_path, 'r') as f:\n",
    "        for file_name in file_names:\n",
    "            print('extracting %s' % file_name)\n",
    "            out_paths.append(f.extract(file_name, path=data_dir))\n",
    "    return out_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_glove_path, = download_and_unzip(\n",
    "#  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "#  'glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'data/sick_filtered_glove2.txt'\n",
    "\n",
    "def filter_glove():\n",
    "    vocab = set()\n",
    "    # Download the full set of unlabeled sentences separated by '|'.\n",
    "    sentence_path = ['sick_train_sentenceA_tree.txt', 'sick_train_sentenceB_tree.txt',\n",
    "                      'sick_trial_sentenceA_tree.txt', 'sick_trial_sentenceB_tree.txt',]\n",
    "    for path in sentence_path:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                # Drop the trailing newline and strip backslashes. Split into words.\n",
    "                vocab.update(line.strip().split())\n",
    "    nread = 0\n",
    "    nwrote = 0\n",
    "    with codecs.open(full_glove_path, encoding='utf-8') as f:\n",
    "        with codecs.open(filtered_glove_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                nread += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    nwrote += 1\n",
    "    print('read %s lines, wrote %s' % (nread, nwrote))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_path):\n",
    "    \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "    print('loading word embeddings from %s' % embedding_path)\n",
    "    weight_vectors = []\n",
    "    word_idx = {}\n",
    "    with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(u' ', 1)\n",
    "            word_idx[word] = len(weight_vectors)\n",
    "            weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "    # Annoying implementation detail; '(' and ')' are replaced by '-LRB-' and\n",
    "    # '-RRB-' respectively in the parse-trees.\n",
    "    if u'(' in word_idx:\n",
    "        word_idx[u'-LRB-'] = word_idx.pop(u'(')\n",
    "    if u')' in word_idx:\n",
    "        word_idx[u'-RRB-'] = word_idx.pop(u')')\n",
    "        \n",
    "    # Random embedding vector for UNKNOWN-WORD.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    word_idx[\"UNKNOWN_WORD\"] = len(weight_vectors)-1\n",
    "    \n",
    "    # Random embedding vector for left_marker.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    word_idx[\"LEFT_MARKER\"] = len(weight_vectors)-1\n",
    "    \n",
    "    # Random embedding vector for right_marker.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    word_idx[\"RIGHT_MARKER\"] = len(weight_vectors)-1\n",
    "    \n",
    "    # Random embedding vector for right_marker.\n",
    "    weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "    word_idx[\"END_MARKER\"] = len(weight_vectors)-1\n",
    "    \n",
    "    return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from sick_filtered_glove2.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word = word_to_idx[\"UNKNOWN_WORD\"]\n",
    "left_marker = word_to_idx[\"LEFT_MARKER\"]\n",
    "right_marker = word_to_idx[\"RIGHT_MARKER\"]\n",
    "end_marker = word_to_idx[\"END_MARKER\"]\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        tree_batches.append(tree_batch)\n",
    "        batch = []\n",
    "        batches.append(batch)\n",
    "        for tree in tree_batch:\n",
    "            result =[]\n",
    "            batch.append(result)\n",
    "            handle_node(tree, result)\n",
    "            if len(result) > max_sequence_length:\n",
    "                max_sequence_length = len(result)\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length + 3:\n",
    "                sentence.append(end_marker)\n",
    "    max_sequence_length += 3\n",
    "    return batches, tree_batches, max_sequence_length\n",
    "                \n",
    "            \n",
    "def handle_node(node, result):\n",
    "    result.append(left_marker)\n",
    "    word = node.data[0]\n",
    "    if word in word_to_idx:\n",
    "        result.append(word_to_idx[word])\n",
    "    else:\n",
    "        result.append(unknown_word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            handle_node(child, result)\n",
    "    result.append(right_marker)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(batches, max_sequence_length):\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length :\n",
    "                sentence.append(end_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_scores_to_p(scores_list):\n",
    "    scores = np.array(scores_list) \n",
    "    num_of_classes = 5 #1, 2, .. , 4, 5\n",
    "    p = np.zeros((len(scores), num_of_classes))\n",
    "    for i, score in enumerate(scores):        \n",
    "        if score == num_of_classes:\n",
    "            p[i][num_of_classes-1] = 1\n",
    "        else:\n",
    "            floor = math.floor(score) \n",
    "            ceil = math.ceil(score) \n",
    "            p[i][floor] = score - floor\n",
    "            p[i][floor-1] = floor - score + 1\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "sequence_len = 100\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 5 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10\n",
    "learn_rate = 0.05\n",
    "max_grad_norm = 5\n",
    "epoch_size = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_train_sentenceA_tree.txt')\n",
    "sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_train_sentenceB_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesA, tree_batchesA, max_sequence_lengthA = create_batches(sentence_treesA, batch_size)\n",
    "batchesB, tree_batchesB, max_sequence_lengthB = create_batches(sentence_treesB, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = max(max_sequence_lengthA, max_sequence_lengthB, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(batchesA, sequence_len)\n",
    "pad_sequences(batchesB, sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batchesA[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores('data/sick_train_score.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "sentence_inputs_A = tf.placeholder(tf.int32, shape=(batch_size, sequence_len), name=\"sentence_inputs_A\")\n",
    "sentence_inputs_B = tf.placeholder(tf.int32, shape=(batch_size, sequence_len), name=\"sentence_inputs_B\")\n",
    "target_score = tf.placeholder(data_type, shape=(batch_size, output_size), name=\"target_scores\")\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "embedded_inputs_A = tf.nn.embedding_lookup(embedding, sentence_inputs_A)\n",
    "embedded_inputs_B = tf.nn.embedding_lookup(embedding, sentence_inputs_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def makeCells():\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        cell1 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        cell2 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    return [cell1, cell2]\n",
    "\n",
    "A =makeCells()\n",
    "B =makeCells()\n",
    "with tf.variable_scope(\"xx\", reuse=tf.AUTO_REUSE):\n",
    "    cellA = tf.contrib.rnn.MultiRNNCell(A, state_is_tuple=True)\n",
    "    initial_stateA = cellA.zero_state(batch_size, data_type)\n",
    "    inputs_A = tf.unstack(embedded_inputs_A, num=sequence_len, axis=1)\n",
    "    unstacked_outputsA, final_stateA = tf.nn.static_rnn(cellA, inputs_A,\\\n",
    "                                              initial_state=initial_stateA,\\\n",
    "                                              dtype=data_type)\n",
    "    outputsA = tf.reshape(tf.concat(unstacked_outputsA, 1), [-1, hidden_size])\n",
    "\n",
    "    cellB = tf.contrib.rnn.MultiRNNCell(B, state_is_tuple=True)\n",
    "    initial_stateB = cellB.zero_state(batch_size, data_type)\n",
    "    inputs_B = tf.unstack(embedded_inputs_B, num=sequence_len, axis=1)\n",
    "    unstacked_outputsB, final_stateB = tf.nn.static_rnn(cellB, inputs_B, initial_state=initial_stateB,\\\n",
    "                                              dtype=data_type)\n",
    "    outputsB = tf.reshape(tf.concat(unstacked_outputsB, 1), [-1, hidden_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsB = tf.reshape(tf.concat(unstacked_outputsB, 1), [25, sequence_len * hidden_size])\n",
    "outputsA = tf.reshape(tf.concat(unstacked_outputsA, 1), [25, sequence_len * hidden_size])\n",
    "\n",
    "layerA = tf.layers.dense(outputsA, hidden_size, name='layerA', reuse=False)\n",
    "layerB = tf.layers.dense(outputsB, hidden_size, name='layerB', reuse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.abs(tf.subtract(layerA, layerB))\n",
    "h_elewise_product = tf.multiply(layerA, layerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) #TODO what's the correct shape??\n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_over_p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_over_p_hat = tf.div(target_score, p_hat)\n",
    "KL = tf.reduce_mean(tf.reduce_sum(tf.multiply(target_score, p_over_p_hat), 1))\n",
    "regularizer = tf.constant(0.0,dtype=data_type)\n",
    "for var in tf.trainable_variables(): \n",
    "    regularizer = tf.add(regularizer, tf.nn.l2_loss(var))\n",
    "loss = KL + reg_lambda*regularizer\n",
    "\n",
    "learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),\n",
    "                                      max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_time = time.time()\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "writer = tf.summary.FileWriter(\"./event_and_checkpoints/\", session.graph)\n",
    "saver = tf.train.Saver()\n",
    "stateA = session.run(initial_stateA)\n",
    "stateB = session.run(initial_stateB)\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for step in range(epoch_size):\n",
    "    feed_dict = {sentence_inputs_A:np.array(batchesA[step]), sentence_inputs_B:np.array(batchesB[step]), \n",
    "                 target_score:np.array(convert_scores_to_p(scores[step]))}\n",
    "    for i, (c, h) in enumerate(initial_stateA):\n",
    "        feed_dict[c] = stateA[i].c\n",
    "        feed_dict[h] = stateA[i].h\n",
    "    for i, (c, h) in enumerate(initial_stateB):\n",
    "        feed_dict[c] = stateB[i].c\n",
    "        feed_dict[h] = stateB[i].h\n",
    "    \n",
    "    fetches = {'loss': loss, 'train_op':train_op}\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    cost = vals[\"loss\"]\n",
    "\n",
    "    costs += cost\n",
    "    iters +=  1\n",
    "\n",
    "    if step % (epoch_size // 100) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                iters * batch_size * max(1, 1) /\n",
    "                (time.time() - start_time)))\n",
    "        print(\"100*Loss %.3f\" % (100*cost))\n",
    "        save_path = saver.save(session, \"./event_and_checkpoints/SemanticRelatednessLSTM.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor 'add_14:0' shape=() dtype=float64>,\n",
       " 'train_op': <tf.Operation 'GradientDescent' type=AssignAdd>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
