{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munashe/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import external_lib as el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_glove_path, = el.download_and_unzip(\n",
    "#  'http://nlp.stanford.edu/data/', 'glove.840B.300d.zip',\n",
    "#  'glove.840B.300d.txt', data_dir = \"./data_sources/glove.6B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_glove_path = 'data/sick_filtered_glove.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2196018 lines, wrote 2465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'guitarist',\n",
       " 'fingers',\n",
       " 'sewing',\n",
       " 'colored',\n",
       " 'groom',\n",
       " 'umbrella',\n",
       " 'have',\n",
       " 'dog,',\n",
       " 'poss',\n",
       " 'drums',\n",
       " 'friends',\n",
       " 'intense',\n",
       " 'leaving',\n",
       " 'buttering',\n",
       " 'people',\n",
       " 'cheerleaders',\n",
       " 'dish',\n",
       " 'toddler',\n",
       " 'vehicles',\n",
       " 'Mud',\n",
       " 'speedily',\n",
       " 'mouse',\n",
       " 'kissing',\n",
       " 'austerely',\n",
       " 'MD',\n",
       " 'dough',\n",
       " 'racer',\n",
       " 'past',\n",
       " 'blue,',\n",
       " 'bicycles',\n",
       " 'radio',\n",
       " 'racquet',\n",
       " 'cleaner',\n",
       " 'ruined',\n",
       " 'Ferrets',\n",
       " 'barbells',\n",
       " 'ceiling',\n",
       " 'binoculars',\n",
       " 'smashing',\n",
       " 'African',\n",
       " 'stirred',\n",
       " 'Two',\n",
       " 'wet',\n",
       " 'npadvmod',\n",
       " 'rose',\n",
       " 'school',\n",
       " 'seeking',\n",
       " 'photograph',\n",
       " 'rainy',\n",
       " 'photo',\n",
       " 'safety',\n",
       " 'smoking',\n",
       " 'tossed',\n",
       " 'grouping',\n",
       " 'guitar,',\n",
       " 'untying',\n",
       " 'goo',\n",
       " 'One',\n",
       " 'boot',\n",
       " 'bale',\n",
       " 'fencing',\n",
       " 'jumping',\n",
       " 'customers',\n",
       " 'substance',\n",
       " 'away',\n",
       " 'following',\n",
       " 'competition',\n",
       " 'wood',\n",
       " 'crowd,',\n",
       " 'teenage',\n",
       " 'night',\n",
       " 'unicycle',\n",
       " 'tackled',\n",
       " 'tricking',\n",
       " 'sheep',\n",
       " 'seashore',\n",
       " 'building',\n",
       " 'noodles',\n",
       " 'CC',\n",
       " 'bun',\n",
       " 'lights',\n",
       " 'Many',\n",
       " 'pong',\n",
       " 'dress',\n",
       " 'filled',\n",
       " 'dyeing',\n",
       " 'way',\n",
       " 'studied',\n",
       " 'rubber',\n",
       " 'room,',\n",
       " 'seated',\n",
       " 'Kids',\n",
       " 'Sunglasses',\n",
       " 'flavored',\n",
       " 'camera',\n",
       " 'scratching',\n",
       " 'clung',\n",
       " 'spouting',\n",
       " 'laughing',\n",
       " 'happy',\n",
       " 'handing',\n",
       " 'officer',\n",
       " 'extracting',\n",
       " 'view',\n",
       " 'flute',\n",
       " 'skunk',\n",
       " 'jumped',\n",
       " 'baskets',\n",
       " 'walked',\n",
       " 'muscular',\n",
       " 'barren',\n",
       " 'gathering',\n",
       " 'gathered',\n",
       " 'cabinet,',\n",
       " 'baby,',\n",
       " 'his',\n",
       " 'spyglass',\n",
       " 'dark',\n",
       " 'cricket',\n",
       " 'eyes',\n",
       " 'murals',\n",
       " 'town',\n",
       " 'children',\n",
       " 'food',\n",
       " 'The',\n",
       " 'loaves',\n",
       " 'masonry',\n",
       " 'TO',\n",
       " 'gnawing',\n",
       " 'skateboarding',\n",
       " 'pianist',\n",
       " 'egg',\n",
       " 'tying',\n",
       " 'defending',\n",
       " 'bride',\n",
       " 'driven',\n",
       " 'sprinkling',\n",
       " 'calendar',\n",
       " 'JJR',\n",
       " 'deer',\n",
       " 'missing',\n",
       " 'leading',\n",
       " 'rider',\n",
       " 'uniform',\n",
       " 'sung',\n",
       " 'relaxing',\n",
       " 'topless,',\n",
       " 'infmod',\n",
       " 'freeing',\n",
       " 'traffic',\n",
       " 'midst',\n",
       " 'cubs',\n",
       " 'kicked',\n",
       " 'handstand',\n",
       " 'girl,',\n",
       " 'class',\n",
       " 'overlooking',\n",
       " 'touchdown',\n",
       " 'clown',\n",
       " 'ferrets',\n",
       " 'elaborate',\n",
       " 'liner',\n",
       " 'coin',\n",
       " 'Christmas',\n",
       " 'messing',\n",
       " 'frame',\n",
       " \"isn't\",\n",
       " 'dyed',\n",
       " 'dry',\n",
       " 'hillside',\n",
       " 'program',\n",
       " 'zucchini',\n",
       " 'coming',\n",
       " 'naked',\n",
       " 'boiling',\n",
       " 'behind',\n",
       " 'courageously',\n",
       " 'wok',\n",
       " 'appealing',\n",
       " 'spanking',\n",
       " 'uninterestedly',\n",
       " 'steps',\n",
       " 'black',\n",
       " 'table',\n",
       " 'bullets',\n",
       " 'slicing',\n",
       " 'spread',\n",
       " 'idling',\n",
       " 'at',\n",
       " 'boring',\n",
       " 'nsubj',\n",
       " 'monkey',\n",
       " 'wrestling',\n",
       " 'gold',\n",
       " 'hiring',\n",
       " 'Somebody',\n",
       " 'her',\n",
       " 'coverall',\n",
       " 'pink',\n",
       " 'artist',\n",
       " 'outdoor',\n",
       " 'columns,',\n",
       " 'activity',\n",
       " 'buying',\n",
       " 'pushing',\n",
       " 'microwave',\n",
       " \"kitten's\",\n",
       " 'grand',\n",
       " 'eat',\n",
       " 'cart',\n",
       " 'acoustic,',\n",
       " 'fence',\n",
       " 'everyone',\n",
       " 'lounging',\n",
       " 'blocks',\n",
       " 'crouching',\n",
       " 'Pieces',\n",
       " 'tomato',\n",
       " 'costumes',\n",
       " 'chubby',\n",
       " 'panel',\n",
       " 'go',\n",
       " 'falling',\n",
       " 'animals',\n",
       " 'retriever',\n",
       " 'asleep',\n",
       " 'foreground',\n",
       " 'really',\n",
       " 'prey.',\n",
       " 'helmeted',\n",
       " 'restraining',\n",
       " 'stream',\n",
       " 'bottle',\n",
       " 'win',\n",
       " 'spotted',\n",
       " 'confidently',\n",
       " 'soldiers',\n",
       " 'gears',\n",
       " 'paws',\n",
       " 'arcade',\n",
       " 'woods',\n",
       " 'word',\n",
       " 'Frisbee',\n",
       " 'swimsuit',\n",
       " 'mud',\n",
       " 'upside',\n",
       " 'riding',\n",
       " 'tricks',\n",
       " 'auxpass',\n",
       " 'shunning',\n",
       " 'concentrated',\n",
       " 'purple',\n",
       " 'preparing',\n",
       " 'boards',\n",
       " 'extravagant',\n",
       " 'done',\n",
       " 'language',\n",
       " 'rapidly',\n",
       " 'morning',\n",
       " 'consists',\n",
       " 'pebbly',\n",
       " 'rhino',\n",
       " 'let',\n",
       " 'beside',\n",
       " 'RB',\n",
       " 'sunny',\n",
       " 'hiker',\n",
       " 'interview',\n",
       " 'takes',\n",
       " 'things',\n",
       " 'dirt',\n",
       " 'none',\n",
       " 'stage',\n",
       " 'riders',\n",
       " 'alone',\n",
       " 'cosmetics',\n",
       " 'Nobody',\n",
       " 'coats',\n",
       " 'preteen',\n",
       " 'doll',\n",
       " 'tennis',\n",
       " 'bandanna',\n",
       " 'mostly',\n",
       " 'pan',\n",
       " 'fur',\n",
       " 'being',\n",
       " 'splash',\n",
       " 'meal',\n",
       " 'field,',\n",
       " 'hands',\n",
       " 'Egyptian',\n",
       " 'distant',\n",
       " 'rocks,',\n",
       " 'backbends',\n",
       " 'licking',\n",
       " 'cardboard',\n",
       " 'graphitized',\n",
       " 'Masked',\n",
       " 'woman,',\n",
       " 'reindeer',\n",
       " 'People',\n",
       " 'amazingly',\n",
       " 'finger',\n",
       " 'leash',\n",
       " 'styling',\n",
       " 'shown',\n",
       " 'fingernails',\n",
       " 'full',\n",
       " 'A',\n",
       " 'trainer',\n",
       " 'Milk',\n",
       " 'parking',\n",
       " 'guy',\n",
       " 'Rugby',\n",
       " 'uniforms',\n",
       " 'animal',\n",
       " 'ice',\n",
       " 'volleyball',\n",
       " 'proudly',\n",
       " 'sad',\n",
       " 'south',\n",
       " 'hind',\n",
       " 'pcomp',\n",
       " 'squirrel',\n",
       " 'provides',\n",
       " 'stairs,',\n",
       " 'beads',\n",
       " 'raining',\n",
       " 'handling',\n",
       " 'aisle',\n",
       " 'after',\n",
       " 'person',\n",
       " 'uncomfortable',\n",
       " 'dogs',\n",
       " 'couple',\n",
       " 'silencer',\n",
       " 'fruit',\n",
       " 'backyard',\n",
       " 'sail',\n",
       " 'packing',\n",
       " 'jacket',\n",
       " 'makes',\n",
       " 'angrily',\n",
       " 'girls',\n",
       " 'checking',\n",
       " 'smeared',\n",
       " 'skateboarder',\n",
       " 'silent',\n",
       " 'fervently',\n",
       " 'erase',\n",
       " 'dresses',\n",
       " 'intensely',\n",
       " 'healthy',\n",
       " 'Music',\n",
       " 'dad',\n",
       " 'sold',\n",
       " 'children,',\n",
       " 'bowl',\n",
       " 'playfully',\n",
       " 'mindedly',\n",
       " 'launching',\n",
       " 'nose',\n",
       " 'suspiciously',\n",
       " 'drawing',\n",
       " 'bald',\n",
       " 'onto',\n",
       " 'blurry',\n",
       " 'rest',\n",
       " 'colors',\n",
       " 'cloak',\n",
       " 'scouts',\n",
       " 'keyboards',\n",
       " 'eggplant',\n",
       " 'black,',\n",
       " 'podium',\n",
       " 'planting',\n",
       " 'feeding',\n",
       " 'magician',\n",
       " 'youngling,',\n",
       " 'paved',\n",
       " 'climb',\n",
       " 'Pink',\n",
       " 'someone',\n",
       " 'drag',\n",
       " 'soccer',\n",
       " 'biker',\n",
       " 'rubbing',\n",
       " 'sign',\n",
       " 'kiddies',\n",
       " 'kayaking',\n",
       " 'ramps',\n",
       " 'lifting',\n",
       " 'palm',\n",
       " 'dices',\n",
       " 'recruits',\n",
       " 'buttons',\n",
       " 'does',\n",
       " 'competitions',\n",
       " 'angels',\n",
       " 'emptying',\n",
       " 'hikers',\n",
       " 'snowsuits',\n",
       " 'couch',\n",
       " 'tricycle',\n",
       " 'listlessly',\n",
       " 'tailing',\n",
       " 'bmx',\n",
       " 'pack,',\n",
       " 'needs',\n",
       " 'csubj',\n",
       " 'eyeing',\n",
       " 'jewelry',\n",
       " 'blonds',\n",
       " 'flies',\n",
       " 'in',\n",
       " 'boulder',\n",
       " 'Oil',\n",
       " 'shooting',\n",
       " 'parrot',\n",
       " \"artist's\",\n",
       " 'racket',\n",
       " 'splashed',\n",
       " 'and',\n",
       " 'bear',\n",
       " 'arms,',\n",
       " 'waterfall',\n",
       " 'open',\n",
       " 'roadway',\n",
       " 'shaved',\n",
       " 'followed',\n",
       " 'fan',\n",
       " 'unstitching',\n",
       " 'revealing',\n",
       " 'handicapped',\n",
       " 'great',\n",
       " 'Asia.',\n",
       " 'swing',\n",
       " 'audience',\n",
       " 'orange,',\n",
       " 'land',\n",
       " 'staring',\n",
       " 'folding',\n",
       " 'plywood',\n",
       " 'Makeup',\n",
       " 'Dough',\n",
       " 'rocky,',\n",
       " 'hat',\n",
       " 'sedan',\n",
       " 'studying',\n",
       " 'shirt',\n",
       " 'dogs,',\n",
       " 'instruments',\n",
       " 'formation',\n",
       " 'not',\n",
       " 'Several',\n",
       " 'sketch',\n",
       " 'bank',\n",
       " 'allowed',\n",
       " 'snake',\n",
       " 'country',\n",
       " 'boardwalk',\n",
       " 'heads',\n",
       " 'calculating',\n",
       " 'as',\n",
       " 'sound',\n",
       " 'air',\n",
       " 'acrobatics',\n",
       " 'drawer',\n",
       " 'training',\n",
       " 'grating',\n",
       " 'badger',\n",
       " 'is',\n",
       " 'football',\n",
       " 'costumed',\n",
       " 'golf',\n",
       " 'clumsily',\n",
       " 'under',\n",
       " 'begging',\n",
       " 'Adults',\n",
       " 'travelling',\n",
       " 'touched',\n",
       " 'beach',\n",
       " 'sprayed',\n",
       " 'flooring',\n",
       " 'fearfully',\n",
       " 'Three',\n",
       " 'bedroom',\n",
       " 'court',\n",
       " 'Arabic',\n",
       " 'turning',\n",
       " 'carpeted',\n",
       " 'mountains',\n",
       " 'grabbing',\n",
       " 'corners',\n",
       " 'amusedly',\n",
       " 'bow',\n",
       " 'weights',\n",
       " 'noisily',\n",
       " 'jump',\n",
       " 'writing',\n",
       " 'starting',\n",
       " 'thrown',\n",
       " 'fire',\n",
       " 'shirtless',\n",
       " 'jetski',\n",
       " '+--',\n",
       " 'malnourished',\n",
       " 'pours',\n",
       " 'lion',\n",
       " 'police',\n",
       " 'cattle',\n",
       " 'showed',\n",
       " 'sharpened',\n",
       " 'ankle',\n",
       " 'harmlessly',\n",
       " 'relaxed',\n",
       " 'gray',\n",
       " 'colorful',\n",
       " 'chairs',\n",
       " 'newspapers',\n",
       " 'cuts',\n",
       " 'drying',\n",
       " 'during',\n",
       " 'fist',\n",
       " 'arrangement',\n",
       " 'nobody',\n",
       " 'eagerly',\n",
       " 'potatoes',\n",
       " 'waving',\n",
       " 'skit',\n",
       " 'motocross',\n",
       " 'blonde',\n",
       " 'whole',\n",
       " 'direction',\n",
       " 'daughter',\n",
       " 'electric',\n",
       " 'crawling',\n",
       " 'chaps',\n",
       " 'blew',\n",
       " 'spices',\n",
       " 'scolding',\n",
       " 'themselves',\n",
       " 'frightened',\n",
       " 'little',\n",
       " 'rcmod',\n",
       " 'tube',\n",
       " 'voraciously',\n",
       " 'mountainside',\n",
       " 'graffiti',\n",
       " 'There',\n",
       " 'benches',\n",
       " 'sausages',\n",
       " 'suits',\n",
       " 'guinea',\n",
       " 'headlights',\n",
       " 'veil',\n",
       " 'pose',\n",
       " 'Small',\n",
       " 'shirts',\n",
       " 'female',\n",
       " 'chopped',\n",
       " 'waiting',\n",
       " 'barking',\n",
       " 'n',\n",
       " 'dying',\n",
       " 'elephant',\n",
       " 'disassembling',\n",
       " 'operating',\n",
       " 'Okra',\n",
       " 'stroked',\n",
       " 'fitting',\n",
       " 'wind',\n",
       " 'strip',\n",
       " 'eyebrow,',\n",
       " 'nail',\n",
       " 'tray',\n",
       " 'frog',\n",
       " 'camouflage.',\n",
       " 'added',\n",
       " 'telephone',\n",
       " 'skull',\n",
       " 'blooming',\n",
       " 'owner',\n",
       " 'faucet',\n",
       " 'girl',\n",
       " 'sweater',\n",
       " 'anchored',\n",
       " 'snowboard',\n",
       " 'buildings',\n",
       " 'attacking',\n",
       " 'herbs',\n",
       " 'cylindrical',\n",
       " 'she',\n",
       " 'fasting',\n",
       " 'others',\n",
       " 'drilling',\n",
       " 'VBD',\n",
       " 'casual',\n",
       " 'wild',\n",
       " 'plants',\n",
       " 'animal,',\n",
       " 'stuntman',\n",
       " 'big',\n",
       " 'face',\n",
       " 'wading',\n",
       " 'barrel',\n",
       " 'Hay',\n",
       " 'folded',\n",
       " 'VBZ',\n",
       " 'suited',\n",
       " 'stadium',\n",
       " 'feathers',\n",
       " 'bikini',\n",
       " 'storing',\n",
       " 'targeting',\n",
       " 'rear',\n",
       " 'removed',\n",
       " 'pants,',\n",
       " 'stove',\n",
       " 'tie',\n",
       " 'tree,',\n",
       " 'note',\n",
       " 'mouths',\n",
       " 'cheers',\n",
       " 'mug',\n",
       " 'stars',\n",
       " 'quickly',\n",
       " 'walks',\n",
       " 'bikers',\n",
       " 'badger,',\n",
       " 'tanking',\n",
       " 'leg',\n",
       " 'classic',\n",
       " 'dunes',\n",
       " 'cameras',\n",
       " 'anyone',\n",
       " 'henna',\n",
       " 'fancy',\n",
       " 'watering',\n",
       " 'khaki',\n",
       " 'water',\n",
       " 'military',\n",
       " 'droplets',\n",
       " 'loading',\n",
       " 'perching',\n",
       " \"man's\",\n",
       " 'by',\n",
       " 'anxiously',\n",
       " 'it.',\n",
       " 'Input:',\n",
       " 'three',\n",
       " 'axe',\n",
       " 'coating',\n",
       " 'skating',\n",
       " 'lone',\n",
       " 'students',\n",
       " 'Pedestrians',\n",
       " 'slip',\n",
       " 'trays',\n",
       " 'art',\n",
       " 'Young',\n",
       " 'fried',\n",
       " 'coat,',\n",
       " 'pair',\n",
       " 'performed',\n",
       " 'bright',\n",
       " 'adults',\n",
       " 'aged',\n",
       " 'boa',\n",
       " 'paper',\n",
       " 'skyline',\n",
       " 'joyful',\n",
       " 'VB',\n",
       " 'cowboy',\n",
       " 'breathlessly',\n",
       " 'discussion',\n",
       " 'shoeless',\n",
       " 'wildly',\n",
       " 'motionlessly',\n",
       " 'carriage',\n",
       " 'shirt,',\n",
       " 'broccoli',\n",
       " 'deleting',\n",
       " 'waste',\n",
       " 'perched',\n",
       " 'shredded',\n",
       " 'race',\n",
       " 'dance',\n",
       " 'mimes',\n",
       " 'garlic',\n",
       " 'driving',\n",
       " 'cane',\n",
       " 'rolling',\n",
       " 'striping',\n",
       " 'pull-ups',\n",
       " 'saucer',\n",
       " 'expl',\n",
       " 'suit',\n",
       " 'making',\n",
       " 'boots',\n",
       " 'farm',\n",
       " 'palace',\n",
       " 'gloved',\n",
       " 'monitor',\n",
       " 'boy',\n",
       " 'field',\n",
       " 'found',\n",
       " 'reflected',\n",
       " 'performer',\n",
       " 'mouth',\n",
       " 'tower',\n",
       " 'contain',\n",
       " 'Grass',\n",
       " 'village',\n",
       " 'various',\n",
       " 'an',\n",
       " 'closed',\n",
       " 'scaring',\n",
       " 'sheet',\n",
       " 'play',\n",
       " 'white,',\n",
       " 'skates',\n",
       " 'amazedly',\n",
       " 'hills',\n",
       " 'spotting',\n",
       " 'concrete',\n",
       " 'receiving',\n",
       " 'nap',\n",
       " 'sprays',\n",
       " 'fireplace',\n",
       " 'ROOT',\n",
       " 'tied',\n",
       " 'statue',\n",
       " 'snapping',\n",
       " 'WRB',\n",
       " 'brick',\n",
       " 'held',\n",
       " 'males',\n",
       " 'played',\n",
       " 'cartoon',\n",
       " 'moving',\n",
       " 'bearded',\n",
       " 'hopping',\n",
       " 'woven',\n",
       " 'getting',\n",
       " 'ledge',\n",
       " 'old',\n",
       " 'midspeech',\n",
       " 'ridden',\n",
       " 'massaging',\n",
       " 'spilling',\n",
       " 'carried',\n",
       " 'snowy',\n",
       " 'guys',\n",
       " 'eyebrows,',\n",
       " 'animatedly',\n",
       " 'workers',\n",
       " 'keeping',\n",
       " 'pasta',\n",
       " 'DT',\n",
       " 'priest',\n",
       " 'drinking',\n",
       " 'footballer',\n",
       " 'skater',\n",
       " 'leapt',\n",
       " 'machine',\n",
       " 'swings',\n",
       " 'guns',\n",
       " 'suit,',\n",
       " 'smearing',\n",
       " 'faced',\n",
       " 'the',\n",
       " 'closing',\n",
       " 'kneeling',\n",
       " 'clove',\n",
       " 'scarves',\n",
       " 'boy,',\n",
       " 'rafting',\n",
       " 'no',\n",
       " 'package',\n",
       " 'stenography',\n",
       " 'crowd',\n",
       " 'marching',\n",
       " 'furiously',\n",
       " 'bare-chested',\n",
       " 'breaking',\n",
       " 'snakes',\n",
       " 'suitcases',\n",
       " 'putting',\n",
       " 'martial',\n",
       " 'guarding',\n",
       " 'opponent',\n",
       " 'burrowed',\n",
       " 'unfolding',\n",
       " 'peel',\n",
       " 'peppering',\n",
       " 'European',\n",
       " 'across',\n",
       " 'young,',\n",
       " 'the/some',\n",
       " 'roped',\n",
       " 'nosing',\n",
       " 'soup',\n",
       " 'sides',\n",
       " 'explorers',\n",
       " 'Water',\n",
       " 'noses',\n",
       " 'filming',\n",
       " 'squeezing',\n",
       " 'toe',\n",
       " 'bull',\n",
       " 'shade',\n",
       " 'pony',\n",
       " 'jet',\n",
       " 'crack',\n",
       " 'Women',\n",
       " 'ducks',\n",
       " 'steak',\n",
       " 'goal',\n",
       " 'tackle',\n",
       " 'birdcage',\n",
       " 'barrier',\n",
       " 'owns',\n",
       " 'bred',\n",
       " 'turtle',\n",
       " 'very',\n",
       " 'shop',\n",
       " 'shrimp',\n",
       " 'skipping',\n",
       " 'rope,',\n",
       " 'advertisement',\n",
       " 'fair',\n",
       " 'circle',\n",
       " 'knives',\n",
       " 'branch',\n",
       " 'cautiously',\n",
       " 'spraying',\n",
       " 'grey',\n",
       " 'buckets',\n",
       " 'Recruits',\n",
       " 'goggles',\n",
       " 'NNS',\n",
       " 'tables',\n",
       " 'ocean',\n",
       " 'ccomp',\n",
       " 'flight',\n",
       " 'something',\n",
       " 'lazily',\n",
       " 'stump',\n",
       " 'eyebrows',\n",
       " 'racing',\n",
       " 'cutting',\n",
       " 'crazily',\n",
       " 'practitioner',\n",
       " 'slowing',\n",
       " 'sparring',\n",
       " 'sadly',\n",
       " 'nothing',\n",
       " 'rail',\n",
       " 'same',\n",
       " 'chasing',\n",
       " 'incline',\n",
       " 'athletically',\n",
       " 'Potatoes',\n",
       " 'stitched',\n",
       " 'power',\n",
       " 'make-up',\n",
       " 'railing',\n",
       " 'stair',\n",
       " 'llama',\n",
       " 'head',\n",
       " 'stack',\n",
       " 'legs',\n",
       " 'hurdle',\n",
       " 'reserved',\n",
       " \"doesn't\",\n",
       " 'protective',\n",
       " 'battling',\n",
       " 'floating',\n",
       " 'walls',\n",
       " 'tomatoes.',\n",
       " 'sofa',\n",
       " 'passionately',\n",
       " 'shaking',\n",
       " 'striking',\n",
       " 'flowing',\n",
       " 'bricks',\n",
       " 'balls',\n",
       " 'wagon',\n",
       " 'stopped',\n",
       " 'burning',\n",
       " 'screen',\n",
       " 'cat',\n",
       " 'drinks',\n",
       " 'bunch',\n",
       " 'beating',\n",
       " 'alongside',\n",
       " 'family,',\n",
       " 'licked',\n",
       " 'surfboards',\n",
       " 'fallen',\n",
       " 'for',\n",
       " 'obstacle',\n",
       " 'parading',\n",
       " 'calmly',\n",
       " 'hand',\n",
       " 'violin',\n",
       " 'presentation',\n",
       " 'requires',\n",
       " 'capsicum',\n",
       " 'clinging',\n",
       " 'mothers',\n",
       " 'field.',\n",
       " 'powerfully',\n",
       " 'upside-down',\n",
       " 'Broccoli',\n",
       " 'elegant',\n",
       " 'arguing',\n",
       " 'acoustic',\n",
       " 'pack',\n",
       " 'operate',\n",
       " 'banana',\n",
       " 'crane',\n",
       " 'digging',\n",
       " 'tap',\n",
       " 'sliced',\n",
       " 'piecing',\n",
       " 'down',\n",
       " 'other',\n",
       " 'jumpsuit',\n",
       " 'wears',\n",
       " 'soft',\n",
       " 'tattered',\n",
       " 'exercise',\n",
       " 'bee',\n",
       " 'there',\n",
       " 'boat',\n",
       " 'experiencing',\n",
       " 'teenager',\n",
       " 'desert',\n",
       " 'helped',\n",
       " 'horse',\n",
       " 'bike,',\n",
       " 'terrain',\n",
       " 'rescues',\n",
       " 'watched',\n",
       " 'match',\n",
       " 'necklace',\n",
       " 'study',\n",
       " 'beret',\n",
       " 'moustache',\n",
       " 'aircraft',\n",
       " 'pitcher',\n",
       " 'kangaroo',\n",
       " 'herd',\n",
       " 'clear',\n",
       " 'flapping',\n",
       " 'dangerously',\n",
       " 'reviving',\n",
       " 'talk',\n",
       " 'completely',\n",
       " 'window',\n",
       " 'picking',\n",
       " 'floats',\n",
       " 'isle',\n",
       " 'cop',\n",
       " 'bottom',\n",
       " 'catch',\n",
       " 'frolicking',\n",
       " 'num',\n",
       " 'quitting',\n",
       " 'sliding',\n",
       " 'ponytail',\n",
       " 'blades',\n",
       " 'stands',\n",
       " 'provide',\n",
       " 'carefully',\n",
       " 'cute',\n",
       " 'skirt',\n",
       " 'shrimps',\n",
       " 'Bikers',\n",
       " 'runner',\n",
       " 'cob',\n",
       " 'physical',\n",
       " 'skinned',\n",
       " 'runners',\n",
       " 'tongues',\n",
       " 'helmets',\n",
       " 'dune',\n",
       " 'different',\n",
       " 'blew,',\n",
       " 'pepper',\n",
       " 'plant',\n",
       " 'charity',\n",
       " 'work',\n",
       " 'ride',\n",
       " 'African,',\n",
       " 'tmod',\n",
       " 'plane,',\n",
       " 'petted',\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_glove_path = 'data_sources/glove.6B/glove.840B.300d.txt'\n",
    "el.filter_glove(full_glove_path, filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from data/sick_filtered_glove.txt\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_to_idx = el.load_embeddings(filtered_glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tree node class\"\"\"\n",
    "class Node(object):\n",
    "    def __init__(self, data, parent=None):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def add_parent(self, obj):\n",
    "        self.parent = obj\n",
    "        \n",
    "    def __str__(self, tabs=0):\n",
    "        #set_trace()\n",
    "        tab_spaces = str.join(\"\", [\" \" for i in range(tabs)])\n",
    "        return tab_spaces + \"+-- Node: \"+ str.join(\"|\", self.data) + \"\\n\"\\\n",
    "                + str.join(\"\\n\", [child.__str__(tabs+2) for child in self.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing inputs\n",
    "Parse indented lines of text into a tree. Children are indented & under the parent\"\"\"\n",
    "#Parse SyntaxtNet output to sentence trees \n",
    "\n",
    "def parse_dep_tree_text(file_name='sick_train_sentenceA_tree.txt'):\n",
    "    all_data=[]\n",
    "    max_children = 0\n",
    "    sentence_trees = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = \"placeholder\"\n",
    "        while not (line.strip() == \"\"):\n",
    "            line = f.readline()\n",
    "            #set_trace()\n",
    "            if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                continue\n",
    "            elif \"ROOT\" in line and (line.index(\"ROOT\") is len(line)-5):\n",
    "                root_tokens = line.split()\n",
    "                current_node = Node(root_tokens)\n",
    "                sentence_trees.append(current_node)\n",
    "                spaces = 0\n",
    "                node_stack = []\n",
    "                #set_trace()\n",
    "                while not line.startswith(\"Input:\"): \n",
    "                    line = f.readline()\n",
    "                    if line.startswith(\"Input:\") or line.startswith(\"Parse:\"):\n",
    "                        break\n",
    "                    elif  line.strip() == \"\":\n",
    "                        break\n",
    "                    else:\n",
    "                        #set_trace()\n",
    "                        if line.index(\"+--\") < spaces:\n",
    "                            while line.index(\"+--\") < spaces:\n",
    "                                current_node, spaces = node_stack.pop()\n",
    "\n",
    "                        if line.index(\"+--\") > spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            new_node = Node(tokens, parent=current_node)\n",
    "                            all_data.append(tokens)\n",
    "                            current_node.add_child(new_node)\n",
    "                            if len(current_node.children)> max_children:\n",
    "                                max_children = len(current_node.children)\n",
    "                            node_stack.append((current_node, spaces))\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "\n",
    "                        elif line.index(\"+--\") == spaces:\n",
    "                            line_copy = line\n",
    "                            line_copy = line_copy.replace(\"|\", \"\")\n",
    "                            line_copy = line_copy.replace(\"+--\", \"\")\n",
    "                            tokens = line_copy.split()\n",
    "                            all_data.append(tokens)\n",
    "                            new_node = Node(tokens, parent=node_stack[-1][0])\n",
    "                            node_stack[-1][0].add_child(new_node)\n",
    "                            if len(node_stack[-1][0].children)> max_children:\n",
    "                                max_children = len(node_stack[-1][0].children)\n",
    "                            current_node = new_node\n",
    "                            spaces = line.index(\"+--\")\n",
    "    return sentence_trees, max_children #a list of the roots nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert trees to a linear representation. Children are listed between the left and right \n",
    "marker in front of the parent. Each word is replaced by its id \"\"\"\n",
    "unknown_word = word_to_idx[\"UNKNOWN_WORD\"]\n",
    "left_marker = word_to_idx[\"LEFT_MARKER\"]\n",
    "right_marker = word_to_idx[\"RIGHT_MARKER\"]\n",
    "end_marker = word_to_idx[\"END_MARKER\"]\n",
    "def create_batches(trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        tree_batches.append(tree_batch)\n",
    "        batch = []\n",
    "        batches.append(batch)\n",
    "        batch_lengths = []\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for tree in tree_batch:\n",
    "            result =[]\n",
    "            batch.append(result)\n",
    "            handle_node(tree, result)\n",
    "            batch_lengths.append(len(result))\n",
    "            if len(result) > max_sequence_length:\n",
    "                max_sequence_length = len(result)\n",
    "    \n",
    "    return batches, tree_batches, max_sequence_length,batches_lengths\n",
    "                \n",
    "            \n",
    "def handle_node(node, result):\n",
    "    result.append(left_marker)\n",
    "    word = node.data[0]\n",
    "    if word in word_to_idx:\n",
    "        result.append(word_to_idx[word])\n",
    "    else:\n",
    "        result.append(unknown_word)\n",
    "        print(\"Unknown word: \"+word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            handle_node(child, result)\n",
    "    result.append(right_marker)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pad sequences with end markers\"\"\"\n",
    "def pad_sequences(batches, max_sequence_length):\n",
    "    for batch in batches:\n",
    "        for sentence in batch:\n",
    "            while len(sentence) < max_sequence_length :\n",
    "                sentence.append(end_marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to load the target scores and split them into batches\"\"\"\n",
    "\n",
    "def load_scores(file_name, batch_size):\n",
    "    score_batches = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        batch = []\n",
    "        for line in f:\n",
    "            if line and float(line):\n",
    "                batch.append(float(line))\n",
    "                \n",
    "            if len(batch)== batch_size: \n",
    "                score_batches.append(batch)\n",
    "                batch = []\n",
    "    return score_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert the score into a set of probabilities over the classes\"\"\"\n",
    "\n",
    "def convert_scores_to_p(scores_list):\n",
    "    scores = np.array(scores_list) \n",
    "    num_of_classes = 5 #1, 2, .. , 4, 5\n",
    "    p = np.zeros((len(scores), num_of_classes))\n",
    "    for i, score in enumerate(scores):        \n",
    "        if score == num_of_classes:\n",
    "            p[i][num_of_classes-1] = 1\n",
    "        else:\n",
    "            floor = math.floor(score) \n",
    "            ceil = math.ceil(score) \n",
    "            p[i][floor] = score - floor\n",
    "            p[i][floor-1] = floor - score + 1\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the sentences into words and convert the words to their ids\n",
    "The set of words by which to split the sentence can be found in the corresponding tree\n",
    "so fetch the set of words first \"\"\"\n",
    "from IPython.core.debugger import set_trace\n",
    "def create_sentence_batches(sentences, trees, tree_batch_size = 25):\n",
    "    max_sequence_length=0\n",
    "    batches = []\n",
    "    batches_lengths= []\n",
    "    tree_batches = []\n",
    "    for i in range(len(trees)//tree_batch_size):\n",
    "        tree_batch = trees[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        sentence_batch = sentences[i*tree_batch_size:(i+1)*tree_batch_size]\n",
    "        batch = []\n",
    "        batch_lengths = []\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "        for j, tree in enumerate(tree_batch):\n",
    "            word_list =[]\n",
    "            get_word_list(tree, word_list)\n",
    "            #set_trace()\n",
    "            sentence_ids = []\n",
    "            batch.append(sentence_ids)\n",
    "            ordered_word_list = sentence_batch[j].replace(\",\", \" , \").replace(\".\", \" . \").replace(\"n't\", \" n't\").replace(\"'s\", \" 's \").split()\n",
    "            \n",
    "            for k in range(len(ordered_word_list)):\n",
    "                word = ordered_word_list[k]\n",
    "                if not word in word_list:\n",
    "                    print(\"missing word: \" + word)\n",
    "                    set_trace()\n",
    "                    for token in word_list:\n",
    "                        if (not token in ordered_word_list) and token in word:\n",
    "                            words = word.replace(token, \" \"+token+\" \").split()\n",
    "                            for half_word in words:\n",
    "                                if half_word in word_to_idx:\n",
    "                                    sentence_ids.append(word_to_idx[half_word])\n",
    "                                else:\n",
    "                                    sentence_ids.append(unknown_word)\n",
    "                            break\n",
    "                elif word in word_to_idx:\n",
    "                    sentence_ids.append(word_to_idx[word])\n",
    "                else:\n",
    "                    sentence_ids.append(unknown_word)\n",
    "            batch_lengths.append(len(sentence_ids))\n",
    "            if len(sentence_ids) > max_sequence_length:\n",
    "                max_sequence_length = len(sentence_ids)\n",
    "    \n",
    "    return batches, max_sequence_length, batches_lengths\n",
    "                \n",
    "            \n",
    "def get_word_list(node, word_list):\n",
    "    word = node.data[0]\n",
    "    word_list.append(word)\n",
    "    if len(node.children)>0:\n",
    "        for child in node.children:\n",
    "            get_word_list(child, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(file):\n",
    "    with open(file, 'r') as f: \n",
    "        contents = f.readlines()\n",
    "        if len(contents[-1].strip())==0:\n",
    "            contents.pop(-1)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "sequence_len = 100\n",
    "num_layers = 2\n",
    "batch_size = 25\n",
    "data_type = tf.float64\n",
    "output_size = 5 #21 classes\n",
    "reg_lambda = 1e-4 #regularization parameter\n",
    "max_children = 10\n",
    "learn_rate = 0.05\n",
    "max_grad_norm = 5\n",
    "epoch_size = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_train_sentenceA_tree.txt')\n",
    "sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_train_sentenceB_tree.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown word: unstitching\n",
      "Unknown word: uninterestedly\n",
      "Unknown word: midspeech\n",
      "Unknown word: bmxs\n",
      "Unknown word: midspeech\n",
      "Unknown word: unstitching\n"
     ]
    }
   ],
   "source": [
    "batchesA, tree_batchesA, max_sequence_lengthA, seq_lenA = create_batches(sentence_treesA, batch_size)\n",
    "batchesB, tree_batchesB, max_sequence_lengthB, seq_lenB = create_batches(sentence_treesB, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = max(max_sequence_lengthA, max_sequence_lengthB)\n",
    "sequence_len_tensor = tf.constant(sequence_len, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(batchesA, sequence_len)\n",
    "pad_sequences(batchesB, sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores('data/sick_train_score.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "sentence_inputs_A = tf.placeholder(tf.int32, shape=(batch_size, sequence_len), name=\"sentence_inputs_A\")\n",
    "sentence_inputs_A_length = tf.placeholder(tf.int32, shape=(batch_size, ), name=\"sentence_inputs_A_length\")\n",
    "sentence_inputs_B = tf.placeholder(tf.int32, shape=(batch_size, sequence_len), name=\"sentence_inputs_B\")\n",
    "sentence_inputs_B_length = tf.placeholder(tf.int32, shape=(batch_size, ), name=\"sentence_inputs_B_length\")\n",
    "target_score = tf.placeholder(data_type, shape=(batch_size, output_size), name=\"target_scores\")\n",
    "embedding = tf.constant(embedding_matrix, dtype=data_type)\n",
    "embedded_inputs_A = tf.nn.embedding_lookup(embedding, sentence_inputs_A)\n",
    "embedded_inputs_B = tf.nn.embedding_lookup(embedding, sentence_inputs_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The model.\"\"\"\n",
    "def makeCells():\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        cell1 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        cell2 = tf.contrib.rnn.BasicLSTMCell(\n",
    "          hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    return [cell1, cell2]\n",
    "\n",
    "A =makeCells()\n",
    "B =makeCells()\n",
    "with tf.variable_scope(\"UnrolledStackedCells\", reuse=tf.AUTO_REUSE):\n",
    "    cellA = tf.contrib.rnn.MultiRNNCell(A, state_is_tuple=True)\n",
    "    outputsA, final_stateA = tf.nn.dynamic_rnn(cellA, embedded_inputs_A,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_A_length)\n",
    "    \n",
    "    cellB = tf.contrib.rnn.MultiRNNCell(B, state_is_tuple=True)\n",
    "    outputsB, final_stateB = tf.nn.dynamic_rnn(cellB, embedded_inputs_B,\\\n",
    "                                              dtype=data_type, sequence_length=sentence_inputs_B_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'UnrolledStackedCells/rnn/transpose_1:0' shape=(25, 108, 300) dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsA = tf.reshape(outputsA, [batch_size, sequence_len * hidden_size])\n",
    "outputsB = tf.reshape(outputsB, [batch_size, sequence_len * hidden_size])\n",
    "\n",
    "with tf.variable_scope(\"dense_layer\"):\n",
    "    layerA = tf.layers.dense(outputsA, hidden_size, name='layer', reuse=False)\n",
    "with tf.variable_scope(\"dense_layer\", reuse=True):\n",
    "    layerB = tf.layers.dense(outputsB, hidden_size, name='layer', reuse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abs_difference = tf.abs(tf.subtract(layerA, layerB))\n",
    "h_elewise_product = tf.multiply(layerA, layerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_h_abs_difference = tf.get_variable(\"W_h_abs_difference\", [hidden_size, output_size], data_type)\n",
    "W_h_elewise_product = tf.get_variable(\"W_h_elewise_product\", [hidden_size, output_size], data_type)\n",
    "B_h = tf.get_variable(\"B_h\", [output_size], data_type)\n",
    "h_s = tf.nn.xw_plus_b(h_abs_difference, W_h_abs_difference, B_h)\n",
    "h_s = tf.add(tf.matmul(h_elewise_product, W_h_elewise_product), h_s)\n",
    "h_s = tf.nn.sigmoid(h_s)\n",
    "\n",
    "W_p = tf.get_variable(\"W_p\", [output_size, output_size], data_type) \n",
    "B_p = tf.get_variable(\"B_p\", [output_size], data_type)\n",
    "p_hat = tf.nn.softmax(tf.nn.xw_plus_b(h_s, W_p, B_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.multiply(p_hat, tf.constant([1,2,3,4,5], dtype=tf.float64))\n",
    "sum_y = tf.reduce_sum(y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_over_p_hat = tf.div(target_score, p_hat)\n",
    "KL = tf.reduce_mean(tf.reduce_sum(tf.multiply(target_score, p_over_p_hat), 1))\n",
    "regularizer = tf.constant(0.0,dtype=data_type)\n",
    "for var in tf.trainable_variables(): \n",
    "    regularizer = tf.add(regularizer, tf.nn.l2_loss(var))\n",
    "loss = KL + reg_lambda*regularizer\n",
    "\n",
    "learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars),\n",
    "                                      max_grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TrainLoss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "test_loss = tf.placeholder(tf.int32, shape=(), name=\"test_loss\")\n",
    "tf.summary.scalar('TestLossAverage', test_loss)\n",
    "tf.summary.scalar('TrainLoss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(batchesA, batchesB, seq_lenA, seq_lenB, scores, output_file):\n",
    "    output_scores=[]\n",
    "    total_loss = 0\n",
    "    for step in range(len(batchesA)):\n",
    "        feed_dict = {sentence_inputs_A:np.array(batchesA[step]), sentence_inputs_A_length:np.array(seq_lenA[step]), \n",
    "                     sentence_inputs_B:np.array(batchesB[step]), sentence_inputs_B_length:np.array(seq_lenB[step]),\n",
    "                     target_score:np.array(convert_scores_to_p(scores[step]))}\n",
    "        \n",
    "\n",
    "        fetches = {'loss': loss, 'y': sum_y}\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        loss_value = vals[\"loss\"]\n",
    "        total_loss += loss_value\n",
    "        output_scores.append(vals[\"y\"])\n",
    "\n",
    "        \n",
    "        #print(\"Loss %.3f\" % (loss))\n",
    "    \n",
    "    #import csv\n",
    "    \"\"\"with open(output_file,'w') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        for batch in output_scores:\n",
    "            for score in batch: \n",
    "                wr.writerow([repr(score)])\"\"\"\n",
    "    return total_loss/len(batchesA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchesA, test_seq_lenA = None, None\n",
    "test_batchesB, test_seq_lenB = None, None\n",
    "test_scores = None\n",
    "def load_tree_test_data_and_test_model(test_batchesA, test_seq_lenA,test_batchesB, test_seq_lenB,test_scores):\n",
    "    if test_batchesA : \n",
    "        average_loss = test_model(test_batchesA, test_batchesB, test_seq_lenA, test_seq_lenB, test_scores, \n",
    "                   \"test_results.txt\")\n",
    "        return average_loss\n",
    "    else:        \n",
    "        sentence_treesA, max_childrenA = parse_dep_tree_text(file_name='data/sick_trial_sentenceA_tree.txt')\n",
    "        sentence_treesB, max_childrenB = parse_dep_tree_text(file_name='data/sick_trial_sentenceB_tree.txt')\n",
    "\n",
    "        test_batchesA, test_tree_batchesA, max_sequence_lengthA, test_seq_lenA = create_batches(sentence_treesA,\n",
    "                                                                                                batch_size)\n",
    "        test_batchesB, test_tree_batchesB, max_sequence_lengthB, test_seq_lenB = create_batches(sentence_treesB,\n",
    "                                                                                                batch_size)\n",
    "\n",
    "        pad_sequences(test_batchesA, sequence_len)\n",
    "        pad_sequences(test_batchesB, sequence_len)\n",
    "\n",
    "        test_scores = load_scores('data/sick_trial_score.txt', batch_size)\n",
    "        average_loss = test_model(test_batchesA, test_batchesB, test_seq_lenA, test_seq_lenB, test_scores, \n",
    "                       \"test_results.txt\")\n",
    "        return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_time = time.time()\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(session, \"./event_and_checkpoints/event_and_checkpoints/SemanticRelatednessLSTM-h.ckpt\")\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./event_and_checkpoints\", session.graph)\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 perplexity: 24165.710 speed: 5 wps\n",
      "100*Loss 1009.269\n",
      "Unknown word: midspeech\n",
      "Avg Test Loss 6.341\n",
      "0.002 perplexity: 84.969 speed: 5 wps\n",
      "100*Loss 318.249\n",
      "Unknown word: midspeech\n",
      "Avg Test Loss 3.330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-75ec6e473883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_op'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for cycles in range(10000):\n",
    "    for step in range(epoch_size//batch_size):\n",
    "        feed_dict = {sentence_inputs_A:np.array(batchesA[step]), sentence_inputs_A_length:np.array(seq_lenA[step]),\n",
    "                     sentence_inputs_B:np.array(batchesB[step]), sentence_inputs_B_length:np.array(seq_lenB[step]),\n",
    "                     target_score:np.array(convert_scores_to_p(scores[step]))}\n",
    "\n",
    "        fetches = {'loss': loss, 'train_op':train_op}\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"loss\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters +=  1\n",
    "\n",
    "        if (cycles == 0 and step == 0 ) or (step % (epoch_size // 100) == 10):\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                    (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                    iters * batch_size * max(1, 1) /\n",
    "                    (time.time() - start_time)))\n",
    "            print(\"100*Loss %.3f\" % (100*cost))\n",
    "            average_test_loss = load_tree_test_data_and_test_model(test_batchesA, test_seq_lenA,test_batchesB, \n",
    "                                                                   test_seq_lenB,test_scores)\n",
    "            print(\"Avg Test Loss %.3f\" % (average_test_loss))\n",
    "            feed_dict[test_loss] = average_test_loss\n",
    "            summary = session.run(merged, feed_dict)\n",
    "            writer.add_summary(summary, cycles*epoch_size//batch_size+step)\n",
    "            save_path = saver.save(session, \"./event_and_checkpoints/SemanticRelatednessLSTM-h.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_batches_A = None\n",
    "sentence_batches_B = None\n",
    "sentence_len_A = None\n",
    "sentence_len_B = None\n",
    "test_scores = None \n",
    "\n",
    "def load_sentence_test_data_and_test_model(sentence_treesA, sentence_treesB, sentenceA_file_name, sentenceB_file_name, \n",
    "                                  score_file_name, output_file_name):\n",
    "    if sentence_batches_A:\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "               output_file_name)\n",
    "    else:\n",
    "        \n",
    "        if not sentenceA_file_name:\n",
    "            sentenceA_file_name = 'data/sick_trial_sentenceA.txt'\n",
    "        if not sentenceB_file_name:\n",
    "            sentenceB_file_name = 'data/sick_trial_sentenceB.txt'\n",
    "        if not score_file_name:\n",
    "            score_file_name = 'data/sick_trial_score.txt'\n",
    "        if not output_file_name:\n",
    "            output_file_name = \"./data/sick_trial_score_sentence_predict.csv\" \n",
    "\n",
    "        sentencesA = load_lines(sentenceA_file_name)\n",
    "        sentencesB = load_lines(sentenceB_file_name)\n",
    "\n",
    "        sentence_batches_A, max_sentence_lengthA, sentence_len_A = create_sentence_batches(sentencesA, sentence_treesA, batch_size)\n",
    "        sentence_batches_B, max_sentence_lengthB, sentence_len_B = create_sentence_batches(sentencesB, sentence_treesB, batch_size)\n",
    "\n",
    "        pad_sequences(sentence_batches_A, sequence_len)\n",
    "        pad_sequences(sentence_batches_B, sequence_len)\n",
    "\n",
    "        test_scores = load_scores(score_file_name, batch_size)\n",
    "\n",
    "        test_model(sentence_batches_A, sentence_batches_B, sentence_len_A, sentence_len_B, test_scores, \n",
    "                   output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
